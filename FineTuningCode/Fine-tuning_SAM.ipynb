{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13267434,"sourceType":"datasetVersion","datasetId":8407603},{"sourceId":598860,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":448545,"modelId":464952}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install sam2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T10:56:16.485474Z","iopub.execute_input":"2025-10-06T10:56:16.485747Z","iopub.status.idle":"2025-10-06T11:01:31.651016Z","shell.execute_reply.started":"2025-10-06T10:56:16.485725Z","shell.execute_reply":"2025-10-06T11:01:31.650033Z"}},"outputs":[{"name":"stdout","text":"Collecting sam2\n  Downloading sam2-1.1.0.tar.gz (152 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.8/152.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: torch>=2.5.1 in /usr/local/lib/python3.11/dist-packages (from sam2) (2.6.0+cu124)\nRequirement already satisfied: torchvision>=0.20.1 in /usr/local/lib/python3.11/dist-packages (from sam2) (0.21.0+cu124)\nRequirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from sam2) (1.26.4)\nRequirement already satisfied: tqdm>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from sam2) (4.67.1)\nCollecting hydra-core>=1.3.2 (from sam2)\n  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\nCollecting iopath>=0.1.10 (from sam2)\n  Downloading iopath-0.1.10.tar.gz (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: pillow>=9.4.0 in /usr/local/lib/python3.11/dist-packages (from sam2) (11.2.1)\nRequirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.11/dist-packages (from hydra-core>=1.3.2->sam2) (2.3.0)\nRequirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from hydra-core>=1.3.2->sam2) (4.9.3)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from hydra-core>=1.3.2->sam2) (25.0)\nRequirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from iopath>=0.1.10->sam2) (4.14.0)\nCollecting portalocker (from iopath>=0.1.10->sam2)\n  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.4->sam2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.4->sam2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.4->sam2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.4->sam2) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.4->sam2) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.4->sam2) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->sam2) (3.18.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->sam2) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->sam2) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->sam2) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.5.1->sam2)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.5.1->sam2)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.5.1->sam2)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.5.1->sam2)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.5.1->sam2)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.5.1->sam2)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.5.1->sam2)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.5.1->sam2)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.5.1->sam2)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->sam2) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->sam2) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->sam2) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.5.1->sam2)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->sam2) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->sam2) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.5.1->sam2) (1.3.0)\nRequirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from omegaconf<2.4,>=2.2->hydra-core>=1.3.2->sam2) (6.0.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.5.1->sam2) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24.4->sam2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24.4->sam2) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.24.4->sam2) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.24.4->sam2) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.24.4->sam2) (2024.2.0)\nDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\nBuilding wheels for collected packages: sam2, iopath\n  Building wheel for sam2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for sam2: filename=sam2-1.1.0-cp311-cp311-linux_x86_64.whl size=473591 sha256=6ba7930f2ec016a1f5b605fb7ed2b683f8154f942e8aacef3d5fbf271318e450\n  Stored in directory: /root/.cache/pip/wheels/d5/c6/ed/164df00d6a31203bc2cd1f65385761825e32a77417b90cc3c7\n  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31527 sha256=f463e874281bfadf83b602638cdb8951c3073ef53f6be1441c726e1601d472b3\n  Stored in directory: /root/.cache/pip/wheels/ba/5e/16/6117f8fe7e9c0c161a795e10d94645ebcf301ccbd01f66d8ec\nSuccessfully built sam2 iopath\nInstalling collected packages: portalocker, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, iopath, hydra-core, nvidia-cusolver-cu12, sam2\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed hydra-core-1.3.2 iopath-0.1.10 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 portalocker-3.2.0 sam2-1.1.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!wget -O /kaggle/working/sam2.1_hiera_tiny.pt \\\n  https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_tiny.pt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T11:01:31.652898Z","iopub.execute_input":"2025-10-06T11:01:31.653135Z","iopub.status.idle":"2025-10-06T11:01:32.456483Z","shell.execute_reply.started":"2025-10-06T11:01:31.653113Z","shell.execute_reply":"2025-10-06T11:01:32.455782Z"}},"outputs":[{"name":"stdout","text":"--2025-10-06 11:01:31--  https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_tiny.pt\nResolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 99.84.118.60, 99.84.118.67, 99.84.118.117, ...\nConnecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|99.84.118.60|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 156008466 (149M) [application/vnd.snesdev-page-table]\nSaving to: ‘/kaggle/working/sam2.1_hiera_tiny.pt’\n\n/kaggle/working/sam 100%[===================>] 148.78M   318MB/s    in 0.5s    \n\n2025-10-06 11:01:32 (318 MB/s) - ‘/kaggle/working/sam2.1_hiera_tiny.pt’ saved [156008466/156008466]\n\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"____\n## Fine-tuning SAM 2.1 (Point-Prompted, Initial Loss)","metadata":{}},{"cell_type":"code","source":"# Fine-tune SAM 2.1 (image) on /kaggle/input/taping-cracks (cracks + taping)\n# Saves to /kaggle/working/ckpts_sam2\n# No data augmentation (only resize). Trains prompt encoder + mask decoder.\n# Inference helper at the end auto-prompts by scanning a small point grid.\n\nimport os, glob, random, json, math\nfrom pathlib import Path\nimport numpy as np\nimport cv2\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\n\n# --- SAM 2.1 imports (install from the official repo beforehand) ---\n# !pip -q install -U \"git+https://github.com/facebookresearch/sam2.git\"\nfrom sam2.build_sam import build_sam2\nfrom sam2.sam2_image_predictor import SAM2ImagePredictor\n\n# =========================\n# CONFIG\n# =========================\nBASE_CANDIDATES = [\n    \"/kaggle/input/taping-cracks\",\n    \"/kaggle/input/taping-cracks/data copy\",\n    \"/kaggle/input/taping-cracks/data_copy\",\n]\nSAVE_DIR   = Path(\"/kaggle/working/ckpts_sam2\")\nIMG_SIZE   = 1024     # SAM2 default training size used in refs\nBATCH_STEPS= 1        # we optimize per-sample; use grad-accum to simulate batch\nEPOCHS     = 10\nLR         = 5e-5\nSEED       = 42\nWORKERS    = 2\nWEIGHT_DECAY = 1e-4\nACCUM_STEPS  = 8\nSTEP_LR_SIZE  = 2000\nSTEP_LR_GAMMA = 0.6\n\n# SAM2.1 config + checkpoint (match pair!)\n# IMPORTANT: use a Hydra config NAME (no .yaml, no absolute path).\nMODEL_CFG = \"configs/sam2.1/sam2.1_hiera_t\"\nCKPT_PATH = \"/kaggle/working/sam2.1_hiera_tiny.pt\"  # put your downloaded ckpt here\n\n# =========================\n# Utils\n# =========================\ndef set_seed(s=SEED):\n    random.seed(s); np.random.seed(s)\n    torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n    torch.backends.cudnn.benchmark=False; torch.backends.cudnn.deterministic=True\n\ndef find_data_root(candidates):\n    for base in candidates:\n        root = Path(base)\n        if (root/\"cracks\").exists() and (root/\"taping\").exists():\n            return root\n        if root.exists():\n            for sub in root.iterdir():\n                if sub.is_dir() and (sub/\"cracks\").exists() and (sub/\"taping\").exists():\n                    return sub\n    raise FileNotFoundError(\"Dataset not found in: \" + \", \".join(candidates))\n\ndef load_pair(img_path, mask_path):\n    img = cv2.cvtColor(cv2.imread(img_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n    if mask_path and os.path.exists(mask_path):\n        m = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n    else:\n        m = np.zeros(img.shape[:2], np.uint8)\n    # hard resize only (no aug)\n    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_LINEAR)\n    m   = cv2.resize(m,   (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n    m   = (m > 127).astype(np.uint8)\n    return img, m\n\ndef sample_points_from_mask(bin_mask, max_points=8):\n    \"\"\"Return K positive points (y, x) uniformly from foreground pixels; empty if none.\"\"\"\n    ys, xs = np.where(bin_mask > 0)\n    if len(ys) == 0:\n        return np.zeros((0,2), np.int32)\n    idx = np.random.choice(len(ys), size=min(max_points, len(ys)), replace=False)\n    pts = np.stack([ys[idx], xs[idx]], axis=1)  # (k,2) in (y,x)\n    return pts\n\ndef _ensure_batched(coords, labels):\n    \"\"\"\n    Ensure shapes compatible with sam_prompt_encoder:\n      coords: (B,K,2), labels: (B,K).\n    Accepts coords (K,2), labels (K,) and adds batch dim; also squeezes (B,K,1)->(B,K).\n    \"\"\"\n    if coords is None or labels is None:\n        return coords, labels\n    if hasattr(coords, \"ndim\") and coords.ndim == 2:\n        coords = coords[None, ...]\n    if hasattr(labels, \"ndim\"):\n        if labels.ndim == 1:\n            labels = labels[None, ...]\n        elif labels.ndim == 3 and labels.shape[-1] == 1:\n            labels = labels.squeeze(-1)\n    return coords, labels\n\n# =========================\n# Dataset\n# =========================\nclass ImgMaskDataset(Dataset):\n    \"\"\"split_dir: .../cracks/train or .../taping/val; no augmentation\"\"\"\n    def __init__(self, split_dir):\n        self.root = Path(split_dir)\n        self.img_dir = self.root/\"images\"\n        self.mask_dir= self.root/\"masks\"\n        self.items=[]\n        for ip in sorted(glob.glob(str(self.img_dir/\"*\"))):\n            base = Path(ip).stem\n            mp=None\n            for ext in (\".png\",\".jpg\",\".jpeg\"):\n                cand = self.mask_dir/f\"{base}{ext}\"\n                if cand.exists(): mp=str(cand); break\n            self.items.append((ip, mp))\n        if not self.items:\n            raise RuntimeError(f\"No images under {self.img_dir}\")\n    def __len__(self): return len(self.items)\n    def __getitem__(self, i):\n        ip, mp = self.items[i]\n        img, mask = load_pair(ip, mp)\n        # points sampled from GT mask (SAM2 expects prompts)\n        pts = sample_points_from_mask(mask, max_points=8)   # (k,2) in (y,x)\n        # Pack numpy -> predictor expects: image HWC uint8, mask uint8\n        return {\"image\": img, \"mask\": mask.astype(np.float32), \"points_yx\": pts, \"path\": ip}\n\ndef build_concat(ds_root: Path, split: str):\n    # crack + taping for given split\n    parts = []\n    for sub in [\"cracks\", \"taping\"]:\n        p = ds_root/sub/split\n        if p.exists():\n            parts.append(ImgMaskDataset(str(p)))\n    if not parts:\n        raise RuntimeError(f\"Missing split={split}\")\n    return ConcatDataset(parts)\n\n# =========================\n# Model / Trainer\n# =========================\ndef bce_loss_from_prob(prob, target):\n    # prob & target: (N, H, W) with prob in [0,1]\n    eps=1e-6\n    return (-target*torch.log(prob+eps) - (1-target)*torch.log(1-prob+eps)).mean()\n\ndef compute_iou(bin_pred, bin_gt, eps=1e-6):\n    inter = (bin_pred & bin_gt).sum(dim=(1,2)).float()\n    union = bin_pred.sum(dim=(1,2)) + bin_gt.sum(dim=(1,2)) - inter\n    return (inter+eps)/(union+eps)\n\ndef train_one_step(predictor, batch, optimizer, scaler, step, device):\n    \"\"\"\n    Recipe:\n    - set_image(image)\n    - _prep_prompts (points as foreground=1)\n    - prompt_encoder + mask_decoder\n    - upsample -> sigmoid -> BCE\n    - score loss aligns predicted IoU with GT IoU\n    \"\"\"\n    img = batch[\"image\"]                 # HWC uint8\n    gt  = batch[\"mask\"]                  # HW float32 (0/1)\n    pts = batch[\"points_yx\"]             # (K,2) or (0,2)\n\n    if pts.shape[0] == 0:\n        return None  # skip unlabeled images\n\n    # IMPORTANT: labels should be 1-D (K,), not (K,1)\n    input_point = np.stack([pts[:,1], pts[:,0]], axis=1).astype(np.float32)  # (K,2) -> (x,y)\n    input_label = np.ones((input_point.shape[0],), dtype=np.float32)         # (K,)\n\n    with torch.amp.autocast(device_type='cuda', enabled=(device.type=='cuda')):\n        predictor.set_image(img)  # encodes the image & caches features\n\n        # Prepare prompts\n        mask_input, unnorm_coords, labels, unnorm_box = predictor._prep_prompts(\n            input_point, input_label, box=None, mask_logits=None, normalize_coords=True\n        )\n        # Force shapes to (B,K,2) and (B,K)\n        unnorm_coords, labels = _ensure_batched(unnorm_coords, labels)\n        if unnorm_coords is None or labels is None or unnorm_coords.shape[1] == 0:\n            return None\n\n        # Encode prompts, decode masks\n        sparse_embeddings, dense_embeddings = predictor.model.sam_prompt_encoder(\n            points=(unnorm_coords, labels), boxes=None, masks=None\n        )\n        batched_mode = unnorm_coords.shape[0] > 1\n        high_res_features = [feat_level[-1].unsqueeze(0) for feat_level in predictor._features[\"high_res_feats\"]]\n\n        low_res_masks, prd_scores, _, _ = predictor.model.sam_mask_decoder(\n            image_embeddings = predictor._features[\"image_embed\"][-1].unsqueeze(0),\n            image_pe         = predictor.model.sam_prompt_encoder.get_dense_pe(),\n            sparse_prompt_embeddings = sparse_embeddings,\n            dense_prompt_embeddings  = dense_embeddings,\n            multimask_output = True,\n            repeat_image     = batched_mode,\n            high_res_features= high_res_features,\n        )\n        # Upsample to original HxW then sigmoid -> prob\n        prd_masks = predictor._transforms.postprocess_masks(low_res_masks, predictor._orig_hw[-1])  # logits\n        prob = torch.sigmoid(prd_masks[:, 0])  # (N=1, H, W)\n\n        gt_t  = torch.from_numpy(gt).to(prob.device)[None, ...]  # (1,H,W)\n\n        seg_loss = bce_loss_from_prob(prob, gt_t)\n\n        # IoU supervision on the score head\n        bin_pred = (prob > 0.5)\n        bin_gt   = (gt_t > 0.5)\n        iou = compute_iou(bin_pred, bin_gt)  # (1,)\n        score_loss = torch.abs(prd_scores[:, 0] - iou).mean()\n\n        loss = seg_loss + 0.05*score_loss\n        loss = loss / ACCUM_STEPS\n\n    scaler.scale(loss).backward()\n    torch.nn.utils.clip_grad_norm_(predictor.model.parameters(), max_norm=1.0)\n\n    if step % ACCUM_STEPS == 0:\n        scaler.step(optimizer)\n        scaler.update()\n        predictor.model.zero_grad()\n\n    return {\n        \"loss\": float(loss.detach().cpu()) * ACCUM_STEPS,\n        \"seg_loss\": float(seg_loss.detach().cpu()),\n        \"iou\": float(iou.detach().cpu().mean())\n    }\n\n# =========================\n# Run\n# =========================\nset_seed(SEED)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSAVE_DIR.mkdir(parents=True, exist_ok=True)\n\nDATA_ROOT = find_data_root(BASE_CANDIDATES)\nprint(\"Using DATA_ROOT:\", DATA_ROOT)\n\ntrain_set = build_concat(DATA_ROOT, \"train\")\nval_set   = build_concat(DATA_ROOT, \"val\")\n\n# Per-sample iteration (SAM2 predictor caches one image at a time cleanly)\ndl_tr = DataLoader(train_set, batch_size=1, shuffle=True,  num_workers=WORKERS, pin_memory=True)\ndl_va = DataLoader(val_set,   batch_size=1, shuffle=False, num_workers=WORKERS, pin_memory=True)\n\n# Build SAM2.1 model + predictor\nassert os.path.exists(CKPT_PATH), f\"Checkpoint not found: {CKPT_PATH}\"\n# IMPORTANT: MODEL_CFG must be a Hydra config NAME (e.g., \"configs/sam2.1/sam2.1_hiera_t\")\nsam2_model = build_sam2(MODEL_CFG, CKPT_PATH, device=device.type)\npredictor = SAM2ImagePredictor(sam2_model)\n\n# Freeze image encoder; train prompt encoder + mask decoder\nif hasattr(predictor.model, \"image_encoder\"):\n    for p in predictor.model.image_encoder.parameters(): p.requires_grad = False\nfor p in predictor.model.sam_prompt_encoder.parameters(): p.requires_grad = True\nfor p in predictor.model.sam_mask_decoder.parameters():  p.requires_grad = True\npredictor.model.sam_prompt_encoder.train(True)\npredictor.model.sam_mask_decoder.train(True)\n\n# Optim & sched\ntrainable_params = [p for p in predictor.model.parameters() if p.requires_grad]\nopt = torch.optim.AdamW(trainable_params, lr=LR, weight_decay=WEIGHT_DECAY)\nsched = torch.optim.lr_scheduler.StepLR(opt, step_size=STEP_LR_SIZE, gamma=STEP_LR_GAMMA)\n\nuse_amp = (device.type == \"cuda\")\nscaler = torch.amp.GradScaler(enabled=use_amp)\n\nbest_iou, best_path = -1.0, None\nglobal_step = 0\n\nfor epoch in range(1, EPOCHS+1):\n    predictor.model.train(True)\n    running, running_iou = 0.0, 0.0\n    pbar = tqdm(dl_tr, desc=f\"Epoch {epoch}/{EPOCHS}\")\n\n    for batch in pbar:\n        # unpack (we purposely keep batch_size=1)\n        feed = {\n            \"image\":      batch[\"image\"][0].numpy(),\n            \"mask\":       batch[\"mask\"][0].numpy(),\n            \"points_yx\":  batch[\"points_yx\"][0].numpy(),  # (K,2)\n            \"path\":       batch[\"path\"][0],\n        }\n        metrics = train_one_step(predictor, feed, opt, scaler, global_step+1, device)\n        global_step += 1\n        if metrics is None: \n            continue\n        running += metrics[\"loss\"]\n        running_iou += metrics[\"iou\"]\n        if global_step % ACCUM_STEPS == 0:\n            sched.step()\n        pbar.set_postfix(loss=f\"{running/(global_step or 1):.4f}\",\n                         iou=f\"{running_iou/(global_step or 1):.3f}\",\n                         lr=f\"{opt.param_groups[0]['lr']:.2e}\")\n\n    # -------- Validation (quick IoU) --------\n    predictor.model.eval()\n    with torch.no_grad(), torch.amp.autocast(device_type='cuda', enabled=use_amp):\n        ious=[]\n        for b in tqdm(dl_va, leave=False, desc=\"Valid\"):\n            img = b[\"image\"][0].numpy()\n            gt  = b[\"mask\"][0].numpy()\n            pts = b[\"points_yx\"][0].numpy()\n            if pts.shape[0]==0: \n                continue\n            input_point = np.stack([pts[:,1], pts[:,0]], axis=1).astype(np.float32)  # (K,2)\n            input_label = np.ones((input_point.shape[0],), dtype=np.float32)         # (K,)\n\n            predictor.set_image(img)\n            mask_input, unnorm_coords, labels, unnorm_box = predictor._prep_prompts(\n                input_point, input_label, box=None, mask_logits=None, normalize_coords=True\n            )\n            unnorm_coords, labels = _ensure_batched(unnorm_coords, labels)\n            if unnorm_coords is None or labels is None or unnorm_coords.shape[1]==0:\n                continue\n\n            sparse_embeddings, dense_embeddings = predictor.model.sam_prompt_encoder(\n                points=(unnorm_coords, labels), boxes=None, masks=None\n            )\n            batched_mode = unnorm_coords.shape[0] > 1\n            high_res_features = [feat_level[-1].unsqueeze(0) for feat_level in predictor._features[\"high_res_feats\"]]\n            low_res_masks, prd_scores, _, _ = predictor.model.sam_mask_decoder(\n                image_embeddings=predictor._features[\"image_embed\"][-1].unsqueeze(0),\n                image_pe=predictor.model.sam_prompt_encoder.get_dense_pe(),\n                sparse_prompt_embeddings=sparse_embeddings,\n                dense_prompt_embeddings=dense_embeddings,\n                multimask_output=True,\n                repeat_image=batched_mode,\n                high_res_features=high_res_features,\n            )\n            prd_masks = predictor._transforms.postprocess_masks(low_res_masks, predictor._orig_hw[-1])\n            prob = torch.sigmoid(prd_masks[:, 0])\n            iou = compute_iou((prob>0.5), torch.from_numpy(gt).to(prob.device)[None,...]>0.5)\n            ious.append(float(iou.mean().detach().cpu()))\n    mean_iou = float(np.mean(ious)) if ious else 0.0\n    print(f\"[Epoch {epoch}] Val mean IoU={mean_iou:.4f}\")\n\n    # Save best\n    if mean_iou > best_iou:\n        best_iou = mean_iou\n        best_path = SAVE_DIR/f\"sam2.1_best_e{epoch}_miou{mean_iou:.4f}.pt\"\n        torch.save(predictor.model.state_dict(), best_path)\n        print(\"Saved:\", best_path)\n\n# Final save\nfinal_path = SAVE_DIR/\"sam2.1_final.pt\"\ntorch.save(predictor.model.state_dict(), final_path)\nprint(\"Final:\", final_path, \"| Best:\", best_path)\n\n# --- Inference helper: auto-prompt via sparse point grid, take best mask by score ---\nwith open(SAVE_DIR/\"inference_helper_sam2.py\", \"w\") as f:\n    f.write(f\"\"\"import os, math, numpy as np, torch, cv2\nfrom sam2.build_sam import build_sam2\nfrom sam2.sam2_image_predictor import SAM2ImagePredictor\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nMODEL_CFG = r\"configs/sam2.1/sam2.1_hiera_t\"   # Hydra config NAME\nCKPT      = r\"{CKPT_PATH}\"\nsam2 = build_sam2(MODEL_CFG, CKPT, device=device.type)\npredictor = SAM2ImagePredictor(sam2)\n\n# Load your finetuned weights\nFT = r\"{str(best_path if best_path else final_path)}\"\nsd = torch.load(FT, map_location=\"cpu\")\npredictor.model.load_state_dict(sd, strict=False)\npredictor.model.eval()\n\ndef _grid_points(h, w, n=9):\n    # n points over the image (rough grid)\n    r = int(math.sqrt(n))\n    ys = np.linspace(h*0.15, h*0.85, r).astype(np.int32)\n    xs = np.linspace(w*0.15, w*0.85, r).astype(np.int32)\n    pts = np.array([(y,x) for y in ys for x in xs], dtype=np.int32)\n    return pts\n\ndef _ensure_batched(coords, labels):\n    if coords is None or labels is None:\n        return coords, labels\n    if hasattr(coords, \"ndim\") and coords.ndim == 2:\n        coords = coords[None, ...]\n    if hasattr(labels, \"ndim\"):\n        if labels.ndim == 1:\n            labels = labels[None, ...]\n        elif labels.ndim == 3 and labels.shape[-1] == 1:\n            labels = labels.squeeze(-1)\n    return coords, labels\n\ndef predict_mask(img_path, out_png, thr=0.5, n_points=9):\n    img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n    ih, iw = img.shape[:2]\n    img_r = cv2.resize(img, ({IMG_SIZE}, {IMG_SIZE}), interpolation=cv2.INTER_LINEAR)\n\n    predictor.set_image(img_r)\n    pts = _grid_points({IMG_SIZE}, {IMG_SIZE}, n=n_points)\n    ipt = np.stack([pts[:,1], pts[:,0]], axis=1).astype(np.float32)  # (K,2)\n    ilb = np.ones((ipt.shape[0],), dtype=np.float32)                 # (K,)\n\n    # Prep prompts and run decoder\n    mask_input, unnorm_coords, labels, unnorm_box = predictor._prep_prompts(\n        ipt, ilb, box=None, mask_logits=None, normalize_coords=True\n    )\n    unnorm_coords, labels = _ensure_batched(unnorm_coords, labels)\n\n    sparse_embeddings, dense_embeddings = predictor.model.sam_prompt_encoder(\n        points=(unnorm_coords, labels), boxes=None, masks=None\n    )\n    batched_mode = unnorm_coords.shape[0] > 1\n    high_res_features = [feat_level[-1].unsqueeze(0) for feat_level in predictor._features[\"high_res_feats\"]]\n    low_res_masks, prd_scores, _, _ = predictor.model.sam_mask_decoder(\n        image_embeddings=predictor._features[\"image_embed\"][-1].unsqueeze(0),\n        image_pe=predictor.model.sam_prompt_encoder.get_dense_pe(),\n        sparse_prompt_embeddings=sparse_embeddings,\n        dense_prompt_embeddings=dense_embeddings,\n        multimask_output=True,\n        repeat_image=batched_mode,\n        high_res_features=high_res_features,\n    )\n    up = predictor._transforms.postprocess_masks(low_res_masks, predictor._orig_hw[-1])  # logits\n    prob = torch.sigmoid(up[:,0]).detach().cpu().numpy()  # [num_prompts, H, W]\n    sc   = prd_scores[:,0].detach().cpu().numpy()         # [num_prompts]\n    best = prob[sc.argmax()]                               # (H,W) in resized space\n    best = cv2.resize((best>thr).astype(\"uint8\")*255, (iw, ih), interpolation=cv2.INTER_NEAREST)\n    cv2.imwrite(out_png, best)\n    return out_png\n# Example:\n# predict_mask(\"/kaggle/input/taping-cracks/taping/val/images/ANY.jpg\", \"/kaggle/working/ANY_sam2.png\")\n\"\"\")\nprint(\"Helper written:\", SAVE_DIR/\"inference_helper_sam2.py\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T21:11:41.689242Z","iopub.execute_input":"2025-10-05T21:11:41.689526Z","iopub.status.idle":"2025-10-05T21:11:41.737213Z","shell.execute_reply.started":"2025-10-05T21:11:41.689498Z","shell.execute_reply":"2025-10-05T21:11:41.736380Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/774796963.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# --- SAM 2.1 imports (install from the official repo beforehand) ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# !pip -q install -U \"git+https://github.com/facebookresearch/sam2.git\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msam2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_sam\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_sam2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msam2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msam2_image_predictor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSAM2ImagePredictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sam2'"],"ename":"ModuleNotFoundError","evalue":"No module named 'sam2'","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"# ================================\n# Test-set evaluation for your fine-tuned SAM 2.1 model\n# Metrics: mean IoU (mIoU) and mean Dice\n# - Matches your train/val recipe: resize to IMG_SIZE and use GT-positive points as prompts\n# - Averages over multiple random GT-point samplings per image to reduce variance\n# - Skips images with empty GT masks (reports count)\n# - Saves a per-image CSV of IoU/Dice to /kaggle/working/test_metrics.csv\n# ================================\n\nimport os, glob, random, math, csv\nfrom pathlib import Path\nimport numpy as np\nimport cv2\nfrom tqdm import tqdm\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\n\n# --- SAM 2.1 imports (assumes installed) ---\n# !pip -q install -U \"git+https://github.com/facebookresearch/sam2.git\"\nfrom sam2.build_sam import build_sam2\nfrom sam2.sam2_image_predictor import SAM2ImagePredictor\n\n# -------------------------\n# CONFIG (align with training)\n# -------------------------\nBASE_CANDIDATES = [\n    \"/kaggle/input/taping-cracks\",\n    \"/kaggle/input/taping-cracks/data copy\",\n    \"/kaggle/input/taping-cracks/data_copy\",\n]\nIMG_SIZE   = 1024\nWORKERS    = 2\nSEED       = 42\n\n# Base SAM2.1 (same pair you used to fine-tune)\nMODEL_CFG  = \"configs/sam2.1/sam2.1_hiera_t\"\nCKPT_PATH  = \"/kaggle/working/sam2.1_hiera_tiny.pt\"  # must exist\n\n# >>> Your fine-tuned weights (given path) <<<\nFT_PATH = \"/kaggle/input/seg_crack_taping_.62iou/pytorch/default/1/sam2.1_best_e6_miou0.6201.pt\"\n\n# Evaluation behavior\nTHRESH                  = 0.5    # binarization of predicted prob\nPROMPTS_PER_IMAGE       = 3      # average N random GT samplings per image\nMAX_POINTS_PER_PROMPT   = 8      # up to K positive points per sampling\nSAVE_PER_IMAGE_CSV      = True\nCSV_PATH                = \"/kaggle/working/test_metrics.csv\"\n\n# -------------------------\n# Utils\n# -------------------------\ndef set_seed(s=SEED):\n    random.seed(s); np.random.seed(s)\n    torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n    torch.backends.cudnn.benchmark=False; torch.backends.cudnn.deterministic=True\n\ndef find_data_root(candidates):\n    for base in candidates:\n        root = Path(base)\n        if (root/\"cracks\").exists() and (root/\"taping\").exists():\n            return root\n        if root.exists():\n            for sub in root.iterdir():\n                if sub.is_dir() and (sub/\"cracks\").exists() and (sub/\"taping\").exists():\n                    return sub\n    raise FileNotFoundError(\"Dataset not found in: \" + \", \".join(candidates))\n\ndef load_pair(img_path, mask_path):\n    img = cv2.cvtColor(cv2.imread(img_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n    if mask_path and os.path.exists(mask_path):\n        m = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n    else:\n        m = np.zeros(img.shape[:2], np.uint8)\n    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_LINEAR)\n    m   = cv2.resize(m,   (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n    m   = (m > 127).astype(np.uint8)\n    return img, m\n\ndef sample_points_from_mask(bin_mask, max_points=8):\n    ys, xs = np.where(bin_mask > 0)\n    if len(ys) == 0:\n        return np.zeros((0,2), np.int32)\n    k = min(max_points, len(ys))\n    idx = np.random.choice(len(ys), size=k, replace=False)\n    return np.stack([ys[idx], xs[idx]], axis=1).astype(np.int32)  # (k,2) in (y,x)\n\ndef _ensure_batched(coords, labels):\n    if coords is None or labels is None:\n        return coords, labels\n    if hasattr(coords, \"ndim\") and coords.ndim == 2:\n        coords = coords[None, ...]\n    if hasattr(labels, \"ndim\"):\n        if labels.ndim == 1:\n            labels = labels[None, ...]\n        elif labels.ndim == 3 and labels.shape[-1] == 1:\n            labels = labels.squeeze(-1)\n    return coords, labels\n\ndef compute_iou_torch(bin_pred, bin_gt, eps=1e-6):\n    inter = (bin_pred & bin_gt).sum(dim=(1,2)).float()\n    union = bin_pred.sum(dim=(1,2)).float() + bin_gt.sum(dim=(1,2)).float() - inter\n    return (inter + eps) / (union + eps)\n\ndef compute_dice_torch(bin_pred, bin_gt, eps=1e-6):\n    inter = (bin_pred & bin_gt).sum(dim=(1,2)).float()\n    card  = bin_pred.sum(dim=(1,2)).float() + bin_gt.sum(dim=(1,2)).float()\n    return (2.0 * inter + eps) / (card + eps)\n\n# -------------------------\n# Dataset\n# -------------------------\nclass ImgMaskDataset(Dataset):\n    \"\"\"split_dir: .../cracks/test or .../taping/test; no augmentation\"\"\"\n    def __init__(self, split_dir):\n        self.root = Path(split_dir)\n        self.img_dir = self.root/\"images\"\n        self.mask_dir= self.root/\"masks\"\n        self.items=[]\n        for ip in sorted(glob.glob(str(self.img_dir/\"*\"))):\n            base = Path(ip).stem\n            mp=None\n            for ext in (\".png\",\".jpg\",\".jpeg\"):\n                cand = self.mask_dir/f\"{base}{ext}\"\n                if cand.exists(): mp=str(cand); break\n            self.items.append((ip, mp))\n        if not self.items:\n            raise RuntimeError(f\"No images under {self.img_dir}\")\n    def __len__(self): return len(self.items)\n    def __getitem__(self, i):\n        ip, mp = self.items[i]\n        img, mask = load_pair(ip, mp)\n        return {\"image\": img, \"mask\": mask.astype(np.float32), \"path\": ip}\n\ndef build_concat(ds_root: Path, split: str):\n    parts = []\n    for sub in [\"cracks\", \"taping\"]:\n        p = ds_root/sub/split\n        if p.exists():\n            parts.append(ImgMaskDataset(str(p)))\n    if not parts:\n        raise RuntimeError(f\"Missing split={split}\")\n    return ConcatDataset(parts)\n\n# -------------------------\n# Load model + data\n# -------------------------\nset_seed(SEED)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nassert os.path.exists(CKPT_PATH), f\"Base checkpoint not found: {CKPT_PATH}\"\nassert os.path.exists(FT_PATH),   f\"Finetuned weights not found: {FT_PATH}\"\n\nsam2_model = build_sam2(MODEL_CFG, CKPT_PATH, device=device.type)\npredictor  = SAM2ImagePredictor(sam2_model)\n\n# Load finetuned weights\nsd = torch.load(FT_PATH, map_location=\"cpu\")\nmissing, unexpected = predictor.model.load_state_dict(sd, strict=False)\nif missing or unexpected:\n    print(f\"[load_state_dict] missing={len(missing)} unexpected={len(unexpected)}\")\n\npredictor.model.eval()\n\nDATA_ROOT = find_data_root(BASE_CANDIDATES)\nprint(\"Using DATA_ROOT:\", DATA_ROOT)\n\n# Prefer explicit 'test' split; fallback to 'val' if not present\ntry:\n    test_set = build_concat(DATA_ROOT, \"test\")\nexcept RuntimeError:\n    print(\"[WARN] 'test' split not found -> using 'val' for evaluation.\")\n    test_set = build_concat(DATA_ROOT, \"val\")\n\ndl_te = DataLoader(test_set, batch_size=1, shuffle=False,\n                   num_workers=WORKERS, pin_memory=True)\n\n# -------------------------\n# Evaluation\n# -------------------------\nuse_amp = (device.type == \"cuda\")\nall_iou, all_dice = [], []\nper_image_rows = []\nskipped_empty = 0\n\nwith torch.no_grad():\n    for b in tqdm(dl_te, desc=\"Evaluate(test)\"):\n        img = b[\"image\"][0].numpy()           # HWC uint8 (resized)\n        gt  = b[\"mask\"][0].numpy().astype(np.uint8)  # (H,W) {0,1}\n        path= b[\"path\"][0]\n\n        # Skip empty GT (no positive; same policy as your val loop)\n        if gt.sum() == 0:\n            skipped_empty += 1\n            continue\n\n        # Cache features once per image\n        with torch.amp.autocast(device_type='cuda', enabled=use_amp):\n            predictor.set_image(img)\n\n        iou_trials, dice_trials = [], []\n        for _ in range(PROMPTS_PER_IMAGE):\n            pts = sample_points_from_mask(gt, max_points=MAX_POINTS_PER_PROMPT)  # (k,2) in (y,x)\n            if pts.shape[0] == 0:\n                break\n\n            input_point = np.stack([pts[:,1], pts[:,0]], axis=1).astype(np.float32)  # (K,2) -> (x,y)\n            input_label = np.ones((input_point.shape[0],), dtype=np.float32)         # (K,)\n\n            with torch.amp.autocast(device_type='cuda', enabled=use_amp):\n                mask_input, unnorm_coords, labels, unnorm_box = predictor._prep_prompts(\n                    input_point, input_label, box=None, mask_logits=None, normalize_coords=True\n                )\n                unnorm_coords, labels = _ensure_batched(unnorm_coords, labels)\n\n                sparse_embeddings, dense_embeddings = predictor.model.sam_prompt_encoder(\n                    points=(unnorm_coords, labels), boxes=None, masks=None\n                )\n                batched_mode = unnorm_coords.shape[0] > 1\n                high_res_features = [feat_level[-1].unsqueeze(0) for feat_level in predictor._features[\"high_res_feats\"]]\n                low_res_masks, prd_scores, _, _ = predictor.model.sam_mask_decoder(\n                    image_embeddings=predictor._features[\"image_embed\"][-1].unsqueeze(0),\n                    image_pe=predictor.model.sam_prompt_encoder.get_dense_pe(),\n                    sparse_prompt_embeddings=sparse_embeddings,\n                    dense_prompt_embeddings=dense_embeddings,\n                    multimask_output=True,\n                    repeat_image=batched_mode,\n                    high_res_features=high_res_features,\n                )\n                up   = predictor._transforms.postprocess_masks(low_res_masks, predictor._orig_hw[-1])  # logits\n                prob = torch.sigmoid(up[:,0])  # (N=1,H,W) on device\n\n            # Threshold + metrics (single mask)\n            pred_bin = (prob > THRESH)\n            gt_t     = torch.from_numpy(gt).to(prob.device)[None, ...].bool()\n\n            iou  = compute_iou_torch(pred_bin, gt_t).mean().item()\n            dice = compute_dice_torch(pred_bin, gt_t).mean().item()\n\n            iou_trials.append(iou)\n            dice_trials.append(dice)\n\n        if len(iou_trials) == 0:\n            skipped_empty += 1\n            continue\n\n        iou_mean  = float(np.mean(iou_trials))\n        dice_mean = float(np.mean(dice_trials))\n\n        all_iou.append(iou_mean)\n        all_dice.append(dice_mean)\n        per_image_rows.append((path, iou_mean, dice_mean))\n\n# -------------------------\n# Report + optional CSV\n# -------------------------\nnum_used = len(all_iou)\nprint(f\"\\nImages used: {num_used} | Skipped (empty GT or no prompts): {skipped_empty}\")\nif num_used > 0:\n    print(f\"Test mIoU : {np.mean(all_iou):.4f}\")\n    print(f\"Test Dice : {np.mean(all_dice):.4f}\")\nelse:\n    print(\"No images evaluated.\")\n\nif SAVE_PER_IMAGE_CSV and num_used > 0:\n    with open(CSV_PATH, \"w\", newline=\"\") as f:\n        w = csv.writer(f)\n        w.writerow([\"image_path\", \"iou\", \"dice\"])\n        w.writerows(per_image_rows)\n    print(\"Per-image metrics saved to:\", CSV_PATH)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T11:03:36.984671Z","iopub.execute_input":"2025-10-06T11:03:36.984982Z","iopub.status.idle":"2025-10-06T11:04:03.255120Z","shell.execute_reply.started":"2025-10-06T11:03:36.984958Z","shell.execute_reply":"2025-10-06T11:04:03.254420Z"}},"outputs":[{"name":"stdout","text":"Using DATA_ROOT: /kaggle/input/taping-cracks/data copy\n","output_type":"stream"},{"name":"stderr","text":"Evaluate(test): 100%|██████████| 165/165 [00:20<00:00,  7.94it/s]","output_type":"stream"},{"name":"stdout","text":"\nImages used: 165 | Skipped (empty GT or no prompts): 0\nTest mIoU : 0.6407\nTest Dice : 0.7747\nPer-image metrics saved to: /kaggle/working/test_metrics.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"_____\n## Fine-tuning SAM 2.1 (Point-Prompted, Improved Loss)","metadata":{}},{"cell_type":"code","source":"# Fine-tune SAM 2.1 (image) on /kaggle/input/taping-cracks (cracks + taping)\n# Saves to /kaggle/working/ckpts_sam2\n# No data augmentation (only resize). Trains prompt encoder + mask decoder.\n# Computes mIoU & Dice on val after each epoch.\n\nimport os, glob, random, math, json\nfrom pathlib import Path\nimport numpy as np\nimport cv2\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\n\n# --- SAM 2.1 imports (install beforehand) ---\n# !pip -q install -U \"git+https://github.com/facebookresearch/sam2.git\"\nfrom sam2.build_sam import build_sam2\nfrom sam2.sam2_image_predictor import SAM2ImagePredictor\n\n# =========================\n# CONFIG\n# =========================\nBASE_CANDIDATES = [\n    \"/kaggle/input/taping-cracks\",\n    \"/kaggle/input/taping-cracks/data copy\",\n    \"/kaggle/input/taping-cracks/data_copy\",\n]\nSAVE_DIR   = Path(\"/kaggle/working/ckpts_sam2\")\nIMG_SIZE   = 1024\nEPOCHS     = 10\nLR         = 5e-5\nSEED       = 42\nWORKERS    = 2\nWEIGHT_DECAY = 1e-4\nCLIP_NORM    = 1.0\n\n# SAM2.1 config + checkpoint (match pair!)\nMODEL_CFG = \"configs/sam2.1/sam2.1_hiera_t\"     # Hydra config NAME\nCKPT_PATH = \"/kaggle/working/sam2.1_hiera_tiny.pt\"  # download & place here\n\n# =========================\n# Utils\n# =========================\ndef set_seed(s=SEED):\n    random.seed(s); np.random.seed(s)\n    torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n    torch.backends.cudnn.benchmark=False; torch.backends.cudnn.deterministic=True\n\ndef find_data_root(candidates):\n    for base in candidates:\n        root = Path(base)\n        if (root/\"cracks\").exists() and (root/\"taping\").exists():\n            return root\n        if root.exists():\n            for sub in root.iterdir():\n                if sub.is_dir() and (sub/\"cracks\").exists() and (sub/\"taping\").exists():\n                    return sub\n    raise FileNotFoundError(\"Dataset not found in: \" + \", \".join(candidates))\n\ndef load_pair(img_path, mask_path):\n    img = cv2.cvtColor(cv2.imread(img_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n    if mask_path and os.path.exists(mask_path):\n        m = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n    else:\n        m = np.zeros(img.shape[:2], np.uint8)\n    # hard resize only (no aug)\n    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_LINEAR)\n    m   = cv2.resize(m,   (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n    m   = (m > 127).astype(np.uint8)\n    return img, m\n\ndef sample_points_from_mask(bin_mask, max_points=8):\n    ys, xs = np.where(bin_mask > 0)\n    if len(ys) == 0:\n        return np.zeros((0,2), np.int32)\n    idx = np.random.choice(len(ys), size=min(max_points, len(ys)), replace=False)\n    pts = np.stack([ys[idx], xs[idx]], axis=1)  # (k,2) in (y,x)\n    return pts\n\ndef _ensure_batched(coords, labels):\n    if coords is None or labels is None:\n        return coords, labels\n    if hasattr(coords, \"ndim\") and coords.ndim == 2:\n        coords = coords[None, ...]\n    if hasattr(labels, \"ndim\"):\n        if labels.ndim == 1:\n            labels = labels[None, ...]\n        elif labels.ndim == 3 and labels.shape[-1] == 1:\n            labels = labels.squeeze(-1)\n    return coords, labels\n\ndef dice_from_prob(prob, target, eps=1e-6):\n    # prob, target: (N, H, W) in [0,1]\n    inter = (prob * target).sum(dim=(-2, -1))\n    union = prob.sum(dim=(-2, -1)) + target.sum(dim=(-2, -1))\n    return (2 * inter + eps) / (union + eps)  # (N,)\n\ndef miou_from_bin(pred_bin, tgt_bin, eps=1e-6):\n    # pred_bin, tgt_bin: bool byte tensors (N,H,W)\n    inter = (pred_bin & tgt_bin).sum(dim=(-2, -1)).float()\n    union = pred_bin.sum(dim=(-2, -1)) + tgt_bin.sum(dim=(-2, -1)) - inter\n    return (inter + eps) / (union + eps)     # (N,)\n\n# =========================\n# Dataset\n# =========================\nclass ImgMaskDataset(Dataset):\n    \"\"\"split_dir: .../cracks/train or .../taping/val; no augmentation\"\"\"\n    def __init__(self, split_dir):\n        self.root = Path(split_dir)\n        self.img_dir = self.root/\"images\"\n        self.mask_dir= self.root/\"masks\"\n        self.items=[]\n        for ip in sorted(glob.glob(str(self.img_dir/\"*\"))):\n            base = Path(ip).stem\n            mp=None\n            for ext in (\".png\",\".jpg\",\".jpeg\",\".bmp\",\".tif\",\".jpg\"):\n                cand = self.mask_dir/f\"{base}{ext}\"\n                if cand.exists(): mp=str(cand); break\n            self.items.append((ip, mp))\n        if not self.items:\n            raise RuntimeError(f\"No images under {self.img_dir}\")\n    def __len__(self): return len(self.items)\n    def __getitem__(self, i):\n        ip, mp = self.items[i]\n        img, mask = load_pair(ip, mp)\n        pts = sample_points_from_mask(mask, max_points=8)   # (k,2) in (y,x)\n        return {\n            \"image\": img,                                 # HWC uint8\n            \"mask\": mask.astype(np.float32),              # HW float32 in {0,1}\n            \"points_yx\": pts,                             # (K,2)\n            \"path\": ip\n        }\n\ndef build_concat(ds_root: Path, split: str):\n    parts = []\n    for sub in [\"cracks\", \"taping\"]:\n        p = ds_root/sub/split\n        if p.exists():\n            parts.append(ImgMaskDataset(str(p)))\n    if not parts:\n        raise RuntimeError(f\"Missing split={split}\")\n    return ConcatDataset(parts)\n\n# =========================\n# Train step\n# =========================\nBCE = torch.nn.BCEWithLogitsLoss()  # autocast-safe\n\ndef train_one_step(predictor, batch, optimizer, scaler, device):\n    \"\"\"\n    - set_image(image)\n    - encode prompts (points)\n    - decode low-res masks -> upsample logits\n    - loss = BCEWithLogits(logits, gt) + Dice(σ(logits), gt) + tiny score loss\n    \"\"\"\n    img = batch[\"image\"]                 # HWC uint8\n    gt  = batch[\"mask\"]                  # HW float32 (0/1)\n    pts = batch[\"points_yx\"]             # (K,2) or (0,2)\n\n    if pts.shape[0] == 0:\n        return None  # skip unlabeled images\n\n    input_point = np.stack([pts[:,1], pts[:,0]], axis=1).astype(np.float32)  # (K,2) x,y\n    input_label = np.ones((input_point.shape[0],), dtype=np.float32)         # (K,)\n\n    with torch.autocast(device_type='cuda', enabled=(device.type=='cuda')):\n        predictor.set_image(img)\n\n        mask_input, unnorm_coords, labels, _ = predictor._prep_prompts(\n            input_point, input_label, box=None, mask_logits=None, normalize_coords=True\n        )\n        unnorm_coords, labels = _ensure_batched(unnorm_coords, labels)\n        if unnorm_coords is None or labels is None or unnorm_coords.shape[1] == 0:\n            return None\n\n        sparse_embeddings, dense_embeddings = predictor.model.sam_prompt_encoder(\n            points=(unnorm_coords, labels), boxes=None, masks=None\n        )\n        batched_mode = unnorm_coords.shape[0] > 1\n        high_res_features = [feat_level[-1].unsqueeze(0)\n                             for feat_level in predictor._features[\"high_res_feats\"]]\n\n        low_res_masks, prd_scores, _, _ = predictor.model.sam_mask_decoder(\n            image_embeddings = predictor._features[\"image_embed\"][-1].unsqueeze(0),\n            image_pe         = predictor.model.sam_prompt_encoder.get_dense_pe(),\n            sparse_prompt_embeddings = sparse_embeddings,\n            dense_prompt_embeddings  = dense_embeddings,\n            multimask_output = True,   # 3 candidates\n            repeat_image     = batched_mode,\n            high_res_features= high_res_features,\n        )\n\n        # Upsample to original size (logits), then select best-of-3 by score\n        up_logits = predictor._transforms.postprocess_masks(low_res_masks, predictor._orig_hw[-1])  # (1,3,H,W)\n        best_idx  = prd_scores.argmax(dim=1)                      # (1,)\n        logits    = up_logits[torch.arange(up_logits.size(0)), best_idx]  # (1,H,W)\n\n        gt_t   = torch.from_numpy(gt).to(logits.device)[None, ...]       # (1,H,W)\n\n        # --- losses ---\n        bce  = BCE(logits, gt_t)                                         # logits!\n        prob = torch.sigmoid(logits)                                     # for metrics & dice\n        dice = 1.0 - dice_from_prob(prob, gt_t).mean()\n\n        # IoU supervision to score head (tiny term)\n        bin_pred = (prob > 0.5)\n        bin_gt   = (gt_t > 0.5)\n        iou = miou_from_bin(bin_pred, bin_gt).mean()\n        score_loss = torch.abs(prd_scores.max(dim=1).values - iou).mean()\n\n        loss = bce + dice + 0.05 * score_loss\n\n    optimizer.zero_grad(set_to_none=True)\n    scaler.scale(loss).backward()\n\n    # Clip grads safely in AMP\n    if CLIP_NORM:\n        scaler.unscale_(optimizer)\n        trainable = [p for p in predictor.model.parameters() if p.requires_grad and p.grad is not None]\n        if trainable:\n            torch.nn.utils.clip_grad_norm_(trainable, CLIP_NORM)\n\n    scaler.step(optimizer)\n    scaler.update()\n\n    return {\n        \"loss\": float(loss.detach().cpu()),\n        \"bce\":  float(bce.detach().cpu()),\n        \"dice\": float(1.0 - dice.detach().cpu()),  # report Dice, not DiceLoss\n        \"miou\": float(iou.detach().cpu())\n    }\n\n# =========================\n# Run\n# =========================\nset_seed(SEED)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSAVE_DIR.mkdir(parents=True, exist_ok=True)\n\nDATA_ROOT = find_data_root(BASE_CANDIDATES)\nprint(\"Using DATA_ROOT:\", DATA_ROOT)\n\ntrain_set = build_concat(DATA_ROOT, \"train\")\nval_set   = build_concat(DATA_ROOT, \"val\")\n\ndl_tr = DataLoader(train_set, batch_size=1, shuffle=True,  num_workers=WORKERS, pin_memory=True)\ndl_va = DataLoader(val_set,   batch_size=1, shuffle=False, num_workers=WORKERS, pin_memory=True)\n\n# Build SAM2.1 model + predictor\nassert os.path.exists(CKPT_PATH), f\"Checkpoint not found: {CKPT_PATH}\"\nsam2_model = build_sam2(MODEL_CFG, CKPT_PATH, device=device.type)\npredictor = SAM2ImagePredictor(sam2_model)\n\n# Freeze image encoder; train prompt encoder + mask decoder\nif hasattr(predictor.model, \"image_encoder\"):\n    for p in predictor.model.image_encoder.parameters(): p.requires_grad = False\nfor p in predictor.model.sam_prompt_encoder.parameters(): p.requires_grad = True\nfor p in predictor.model.sam_mask_decoder.parameters():  p.requires_grad = True\npredictor.model.sam_prompt_encoder.train(True)\npredictor.model.sam_mask_decoder.train(True)\n\n# Optim\ntrainable_params = [p for p in predictor.model.parameters() if p.requires_grad]\noptimizer = torch.optim.AdamW(trainable_params, lr=LR, weight_decay=WEIGHT_DECAY)\n\nuse_amp = (device.type == \"cuda\")\nscaler = torch.amp.GradScaler(enabled=use_amp)\n\nbest_miou, best_path = -1.0, None\n\nfor epoch in range(1, EPOCHS+1):\n    predictor.model.train(True)\n    running = {\"loss\":0.0, \"bce\":0.0, \"dice\":0.0, \"miou\":0.0}\n    pbar = tqdm(dl_tr, desc=f\"Epoch {epoch}/{EPOCHS}\")\n\n    for batch in pbar:\n        feed = {\n            \"image\":      batch[\"image\"][0].numpy(),\n            \"mask\":       batch[\"mask\"][0].numpy(),\n            \"points_yx\":  batch[\"points_yx\"][0].numpy(),\n            \"path\":       batch[\"path\"][0],\n        }\n        metrics = train_one_step(predictor, feed, optimizer, scaler, device)\n        if metrics is None:\n            continue\n        for k in running: running[k] += metrics.get(k, 0.0)\n        steps = max(1, sum(1 for _ in [metrics]))\n        pbar.set_postfix(\n            loss=f\"{running['loss']/steps:.4f}\",\n            dice=f\"{running['dice']/steps:.3f}\",\n            miou=f\"{running['miou']/steps:.3f}\",\n            lr=f\"{optimizer.param_groups[0]['lr']:.2e}\"\n        )\n\n    # -------- Validation (mIoU & Dice) --------\n    predictor.model.eval()\n    val_ious, val_dices = [], []\n    with torch.no_grad(), torch.autocast(device_type='cuda', enabled=use_amp):\n        for b in tqdm(dl_va, leave=False, desc=\"Valid\"):\n            img = b[\"image\"][0].numpy()\n            gt  = b[\"mask\"][0].numpy()\n            pts = b[\"points_yx\"][0].numpy()\n            if pts.shape[0]==0: \n                continue\n            input_point = np.stack([pts[:,1], pts[:,0]], axis=1).astype(np.float32)\n            input_label = np.ones((input_point.shape[0],), dtype=np.float32)\n\n            predictor.set_image(img)\n            mask_input, unnorm_coords, labels, _ = predictor._prep_prompts(\n                input_point, input_label, box=None, mask_logits=None, normalize_coords=True\n            )\n            unnorm_coords, labels = _ensure_batched(unnorm_coords, labels)\n            if unnorm_coords is None or labels is None or unnorm_coords.shape[1]==0:\n                continue\n\n            sparse_embeddings, dense_embeddings = predictor.model.sam_prompt_encoder(\n                points=(unnorm_coords, labels), boxes=None, masks=None\n            )\n            batched_mode = unnorm_coords.shape[0] > 1\n            high_res_features = [feat_level[-1].unsqueeze(0) for feat_level in predictor._features[\"high_res_feats\"]]\n            low_res_masks, prd_scores, _, _ = predictor.model.sam_mask_decoder(\n                image_embeddings=predictor._features[\"image_embed\"][-1].unsqueeze(0),\n                image_pe=predictor.model.sam_prompt_encoder.get_dense_pe(),\n                sparse_prompt_embeddings=sparse_embeddings,\n                dense_prompt_embeddings=dense_embeddings,\n                multimask_output=True,\n                repeat_image=batched_mode,\n                high_res_features=high_res_features,\n            )\n            up_logits = predictor._transforms.postprocess_masks(low_res_masks, predictor._orig_hw[-1])  # (1,3,H,W)\n            best_idx  = prd_scores.argmax(dim=1)\n            logits    = up_logits[torch.arange(up_logits.size(0)), best_idx]            # (1,H,W)\n            prob      = torch.sigmoid(logits)                                           # (1,H,W)\n            gt_t      = torch.from_numpy(gt).to(prob.device)[None, ...]                 # (1,H,W)\n            val_ious.append(float(miou_from_bin(prob>0.5, gt_t>0.5).mean().cpu()))\n            val_dices.append(float(dice_from_prob(prob, gt_t).mean().cpu()))\n\n    mIoU = float(np.mean(val_ious)) if val_ious else 0.0\n    mDice= float(np.mean(val_dices)) if val_dices else 0.0\n    print(f\"[Epoch {epoch}] Val mIoU={mIoU:.4f}  Dice={mDice:.4f}\")\n\n    # Save best\n    if mIoU > best_miou:\n        best_miou = mIoU\n        best_path = SAVE_DIR/f\"sam2.1_best_e{epoch}_miou{mIoU:.4f}.pt\"\n        torch.save(predictor.model.state_dict(), best_path)\n        print(\"Saved:\", best_path)\n\n# Final save\nfinal_path = SAVE_DIR/\"sam2.1_final.pt\"\ntorch.save(predictor.model.state_dict(), final_path)\nprint(\"Final:\", final_path, \"| Best:\", best_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T21:22:46.493222Z","iopub.execute_input":"2025-10-05T21:22:46.493557Z","iopub.status.idle":"2025-10-06T01:59:13.414400Z","shell.execute_reply.started":"2025-10-05T21:22:46.493532Z","shell.execute_reply":"2025-10-06T01:59:13.413610Z"}},"outputs":[{"name":"stdout","text":"Using DATA_ROOT: /kaggle/input/taping-cracks/data copy\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 17595/17595 [28:16<00:00, 10.37it/s, dice=12212.965, loss=7077.9204, lr=5.00e-05, miou=10087.354]\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 1] Val mIoU=0.6542  Dice=0.7641\nSaved: /kaggle/working/ckpts_sam2/sam2.1_best_e1_miou0.6542.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 17595/17595 [27:54<00:00, 10.51it/s, dice=12762.772, loss=6278.5104, lr=5.00e-05, miou=10686.844]\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 2] Val mIoU=0.6774  Dice=0.7847\nSaved: /kaggle/working/ckpts_sam2/sam2.1_best_e2_miou0.6774.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 17595/17595 [28:09<00:00, 10.42it/s, dice=12994.349, loss=5953.8207, lr=5.00e-05, miou=10951.218]\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 3] Val mIoU=0.6865  Dice=0.7891\nSaved: /kaggle/working/ckpts_sam2/sam2.1_best_e3_miou0.6865.pt\n","output_type":"stream"},{"name":"stderr","text":"                                                        .90it/s, dice=5709.616, loss=2531.5772, lr=5.00e-05, miou=4828.374]\r","output_type":"stream"},{"name":"stdout","text":"[Epoch 5] Val mIoU=0.6888  Dice=0.7919\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 17595/17595 [26:10<00:00, 11.20it/s, dice=13340.971, loss=5478.1312, lr=5.00e-05, miou=11364.985]\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 6] Val mIoU=0.6997  Dice=0.8014\nSaved: /kaggle/working/ckpts_sam2/sam2.1_best_e6_miou0.6997.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 17595/17595 [25:53<00:00, 11.33it/s, dice=13445.321, loss=5331.1157, lr=5.00e-05, miou=11490.915]\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 7] Val mIoU=0.7011  Dice=0.8007\nSaved: /kaggle/working/ckpts_sam2/sam2.1_best_e7_miou0.7011.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 17595/17595 [25:57<00:00, 11.29it/s, dice=13522.615, loss=5228.5929, lr=5.00e-05, miou=11585.544]\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 8] Val mIoU=0.7024  Dice=0.8016\nSaved: /kaggle/working/ckpts_sam2/sam2.1_best_e8_miou0.7024.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 17595/17595 [26:13<00:00, 11.18it/s, dice=13595.385, loss=5136.2944, lr=5.00e-05, miou=11676.207]\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 9] Val mIoU=0.7012  Dice=0.8009\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 17595/17595 [25:48<00:00, 11.36it/s, dice=13662.221, loss=5046.8054, lr=5.00e-05, miou=11759.151]\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 10] Val mIoU=0.7011  Dice=0.7990\nFinal: /kaggle/working/ckpts_sam2/sam2.1_final.pt | Best: /kaggle/working/ckpts_sam2/sam2.1_best_e8_miou0.7024.pt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}