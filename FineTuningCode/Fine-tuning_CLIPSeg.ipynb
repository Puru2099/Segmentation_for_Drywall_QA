{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13267434,"sourceType":"datasetVersion","datasetId":8407603}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Kaggle: install dependencies\n!pip install -q \"transformers>=4.41\" albumentations opencv-python-headless timm\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-05T09:57:48.971338Z","iopub.execute_input":"2025-10-05T09:57:48.971526Z","iopub.status.idle":"2025-10-05T09:59:04.092454Z","shell.execute_reply.started":"2025-10-05T09:57:48.971503Z","shell.execute_reply":"2025-10-05T09:59:04.091758Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# CLIPSeg fine-tuning on /kaggle/input/taping-cracks (cracks + taping)\n# Outputs to /kaggle/working/ckpts_clipseg\n\nimport os, glob, random, json\nfrom pathlib import Path\nimport numpy as np, cv2\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\n\nimport albumentations as A\nfrom transformers import (\n    CLIPSegProcessor,\n    CLIPSegForImageSegmentation,\n    get_cosine_schedule_with_warmup,\n)\n\n# =========================\n# CONFIG\n# =========================\nBASE_CANDIDATES = [\n    \"/kaggle/input/taping-cracks\",\n    \"/kaggle/input/taping-cracks/data copy\",\n    \"/kaggle/input/taping-cracks/data_copy\",\n]\nSAVE_DIR  = Path(\"/kaggle/working/ckpts_clipseg\")\nIMG_SIZE  = 512\nBATCH     = 6\nEPOCHS    = 15\nLR        = 5e-5\nSEED      = 42\nWORKERS   = 2\nWEIGHT_DECAY = 1e-4\nWARMUP_FRAC  = 0.05\nCLIP_GRAD_NORM = 1.0\n\nCRACK_PROMPTS_TRAIN  = [\"segment crack\", \"segment wall crack\"]\nCRACK_PROMPTS_VAL    = [\"segment crack\"]\nTAPING_PROMPTS_TRAIN = [\"segment taping area\", \"segment drywall seam\", \"segment joint/tape\"]\nTAPING_PROMPTS_VAL   = [\"segment taping area\"]\n\n# =========================\n# Utils\n# =========================\ndef set_seed(s=SEED):\n    random.seed(s); np.random.seed(s)\n    torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n    torch.backends.cudnn.benchmark=False; torch.backends.cudnn.deterministic=True\n\ndef find_data_root(candidates):\n    for base in candidates:\n        root = Path(base)\n        if (root/\"cracks\").exists() and (root/\"taping\").exists():\n            return root\n        if root.exists():\n            for sub in root.iterdir():\n                if sub.is_dir() and (sub/\"cracks\").exists() and (sub/\"taping\").exists():\n                    return sub\n    raise FileNotFoundError(\"Dataset not found in: \" + \", \".join(candidates))\n\n# ---- FIXED AUGMENTATIONS ----\ndef build_aug(img_size: int, for_cracks: bool):\n    \"\"\"\n    Keep geometry light for cracks (thin), a bit stronger for taping.\n    Use only Albumentations transforms that are broadly available on Kaggle.\n    \"\"\"\n    common_photo = [\n        A.RandomBrightnessContrast(0.2, 0.2, p=0.7),\n        A.RandomGamma(gamma_limit=(90, 110), p=0.5),   # widely available\n        A.GaussianBlur(blur_limit=(3, 3), p=0.2),      # mild blur instead of GaussNoise\n    ]\n    if for_cracks:\n        return A.Compose([\n            A.HorizontalFlip(p=0.5),\n            A.Rotate(limit=5, border_mode=cv2.BORDER_CONSTANT, p=0.4),\n            A.Affine(scale=(0.95, 1.05), translate_percent=0.04, p=0.4),\n            *common_photo,\n            A.Resize(img_size, img_size, interpolation=cv2.INTER_LINEAR),\n        ])\n    else:\n        return A.Compose([\n            A.HorizontalFlip(p=0.5),\n            A.Rotate(limit=10, border_mode=cv2.BORDER_CONSTANT, p=0.5),\n            A.Affine(scale=(0.9, 1.1), translate_percent=0.05, p=0.5),\n            *common_photo,\n            A.Resize(img_size, img_size, interpolation=cv2.INTER_LINEAR),\n        ])\n\n\nclass PromptedSegDataset(Dataset):\n    \"\"\" split_dir: .../cracks/train or .../taping/val \"\"\"\n    def __init__(self, split_dir, prompts, img_size, for_cracks, augment, processor):\n        self.root = Path(split_dir)\n        self.img_dir = self.root/\"images\"\n        self.mask_dir= self.root/\"masks\"\n        self.prompts = prompts\n        self.processor = processor\n        self.tf = build_aug(img_size, for_cracks) if augment else A.Compose([A.Resize(img_size, img_size, interpolation=cv2.INTER_LINEAR)])\n        self.items=[]\n        img_paths = sorted(glob.glob(str(self.img_dir/\"*\")))\n        if not img_paths: raise RuntimeError(f\"No images in {self.img_dir}\")\n        for ip in img_paths:\n            base = Path(ip).stem\n            mp=None\n            for ext in (\".png\",\".jpg\",\".jpeg\"):\n                cand = self.mask_dir/f\"{base}{ext}\"\n                if cand.exists(): mp=str(cand); break\n            self.items.append((ip, mp))\n    def __len__(self): return len(self.items)\n    def __getitem__(self, i):\n        ip, mp = self.items[i]\n        img = cv2.cvtColor(cv2.imread(ip, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n        if mp and os.path.exists(mp):\n            mask = cv2.imread(mp, cv2.IMREAD_GRAYSCALE)\n        else:\n            mask = np.zeros(img.shape[:2], np.uint8)\n        mask = (mask>127).astype(np.uint8)*255\n        out = self.tf(image=img, mask=mask); img, mask = out[\"image\"], out[\"mask\"]\n        prompt = random.choice(self.prompts)\n        proc = self.processor(text=[prompt], images=[Image.fromarray(img)], padding=\"max_length\", return_tensors=\"pt\")\n        return {\n            \"pixel_values\": proc.pixel_values[0],\n            \"input_ids\": proc.input_ids[0],\n            \"attention_mask\": proc.attention_mask[0],\n            \"mask\": torch.from_numpy((mask>127).astype(np.float32)),\n            \"prompt\": prompt, \"path\": ip,\n        }\n\ndef dice_coeff(prob, tgt, eps=1e-6):\n    if prob.ndim==4: prob=prob[:,0]\n    pred=(prob>0.5).float()\n    inter=(pred*tgt).sum(dim=[1,2]); union=pred.sum(dim=[1,2])+tgt.sum(dim=[1,2])\n    return ((2*inter+eps)/(union+eps)).mean().item()\n\ndef miou(prob, tgt, eps=1e-6):\n    if prob.ndim==4: prob=prob[:,0]\n    pred=(prob>0.5).float()\n    inter=(pred*tgt).sum(dim=[1,2]); union=pred.sum(dim=[1,2])+tgt.sum(dim=[1,2])-inter\n    return ((inter+eps)/(union+eps)).mean().item()\n\nclass BCEDice(torch.nn.Module):\n    def __init__(self, w=0.5):\n        super().__init__(); self.w=w; self.bce=torch.nn.BCEWithLogitsLoss()\n    def forward(self, logits, tgt):\n        bce=self.bce(logits.squeeze(1), tgt)\n        prob=torch.sigmoid(logits).squeeze(1)\n        inter=(prob*tgt).sum(dim=[1,2]); union=prob.sum(dim=[1,2])+tgt.sum(dim=[1,2])\n        dice=1-(2*inter+1e-6)/(union+1e-6)\n        return self.w*bce+(1-self.w)*dice.mean()\n\n# =========================\n# Train\n# =========================\nset_seed(SEED)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSAVE_DIR.mkdir(parents=True, exist_ok=True)\n\nDATA_ROOT = find_data_root(BASE_CANDIDATES)\nprint(\"Using DATA_ROOT:\", DATA_ROOT)\n\nprocessor = CLIPSegProcessor.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\nmodel = CLIPSegForImageSegmentation.from_pretrained(\"CIDAS/clipseg-rd64-refined\").to(device)\n\nds_tr = ConcatDataset([\n    PromptedSegDataset(str(DATA_ROOT/\"cracks\"/\"train\"), CRACK_PROMPTS_TRAIN,  IMG_SIZE, True,  True,  processor),\n    PromptedSegDataset(str(DATA_ROOT/\"taping\"/\"train\"), TAPING_PROMPTS_TRAIN, IMG_SIZE, False, True,  processor),\n])\nds_va_cr = PromptedSegDataset(str(DATA_ROOT/\"cracks\"/\"val\"),  CRACK_PROMPTS_VAL,  IMG_SIZE, True,  False, processor)\nds_va_tp = PromptedSegDataset(str(DATA_ROOT/\"taping\"/\"val\"), TAPING_PROMPTS_VAL, IMG_SIZE, False, False, processor)\nds_va = ConcatDataset([ds_va_cr, ds_va_tp])\n\ndl_tr = DataLoader(ds_tr, batch_size=BATCH, shuffle=True,  num_workers=WORKERS, pin_memory=True, drop_last=True)\ndl_va = DataLoader(ds_va, batch_size=BATCH, shuffle=False, num_workers=WORKERS, pin_memory=True)\n\nopt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\nsteps = EPOCHS * len(dl_tr); warm = int(WARMUP_FRAC * steps)\nsched = get_cosine_schedule_with_warmup(opt, num_warmup_steps=warm, num_training_steps=steps)\nloss_fn = BCEDice(0.5)\nuse_amp = torch.cuda.is_available()\nscaler = torch.amp.GradScaler(device=\"cuda\") if use_amp else None\n\nbest_dice, best_path = -1.0, None\nfor epoch in range(1, EPOCHS+1):\n    model.train()\n    running = 0.0\n    pbar = tqdm(dl_tr, total=len(dl_tr), desc=f\"Epoch {epoch}/{EPOCHS}\")\n\n    for step, b in enumerate(pbar, 1):   # start at 1 to avoid div by zero\n        px   = b[\"pixel_values\"].to(device, non_blocking=True)\n        ids  = b[\"input_ids\"].to(device, non_blocking=True)\n        attn = b[\"attention_mask\"].to(device, non_blocking=True)\n        gt   = b[\"mask\"].to(device, non_blocking=True)\n\n        opt.zero_grad(set_to_none=True)\n        with torch.amp.autocast(device_type=\"cuda\", enabled=use_amp):\n            out = model(pixel_values=px, input_ids=ids, attention_mask=attn)\n            logits = out.logits\n            gt_rs  = F.interpolate(gt.unsqueeze(1), size=logits.shape[-2:], mode=\"nearest\").squeeze(1)\n            loss   = loss_fn(logits, gt_rs)\n\n        if use_amp:\n            scaler.scale(loss).backward()\n            if CLIP_GRAD_NORM:\n                scaler.unscale_(opt)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_GRAD_NORM)\n            scaler.step(opt); scaler.update()\n        else:\n            loss.backward()\n            if CLIP_GRAD_NORM:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_GRAD_NORM)\n            opt.step()\n\n        sched.step()\n        running += loss.item()\n        pbar.set_postfix(avg_loss=f\"{running/step:.4f}\")  # <- uses step, not pbar.n\n\n    # ------- validation unchanged -------\n    model.eval(); dices=[]; ious=[]\n    with torch.no_grad():\n        for b in tqdm(dl_va, leave=False, desc=\"Valid\"):\n            px   = b[\"pixel_values\"].to(device)\n            ids  = b[\"input_ids\"].to(device)\n            attn = b[\"attention_mask\"].to(device)\n            gt   = b[\"mask\"].to(device)\n            out  = model(pixel_values=px, input_ids=ids, attention_mask=attn)\n            logits = out.logits\n            gt_rs  = F.interpolate(gt.unsqueeze(1), size=logits.shape[-2:], mode=\"nearest\").squeeze(1)\n            prob   = torch.sigmoid(logits)\n            dices.append(dice_coeff(prob, gt_rs))\n            ious.append(miou(prob, gt_rs))\n    d, m = float(np.mean(dices)), float(np.mean(ious))\n    print(f\"[Epoch {epoch}] Val Dice={d:.4f}  mIoU={m:.4f}\")\n\n    if d > best_dice:\n        best_dice = d\n        best_path = SAVE_DIR/f\"clipseg_best_e{epoch}_dice{d:.4f}.pt\"\n        torch.save({\"model\": model.state_dict(), \"epoch\": epoch, \"dice\": d, \"miou\": m}, best_path)\n        print(\"Saved:\", best_path)\n\nfinal_path = SAVE_DIR/\"clipseg_final.pt\"\ntorch.save({\"model\": model.state_dict()}, final_path)\nprint(\"Final:\", final_path, \"| Best:\", best_path)\n\nwith open(SAVE_DIR/\"inference_helper.py\", \"w\") as f:\n    f.write(f\"\"\"import torch, cv2\nfrom PIL import Image\nfrom transformers import CLIPSegProcessor, CLIPSegForImageSegmentation\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprocessor = CLIPSegProcessor.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\nmodel = CLIPSegForImageSegmentation.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\nckpt = r\"{str(best_path if best_path else final_path)}\"\nsd = torch.load(ckpt, map_location=\"cpu\"); sd = sd.get(\"model\", sd)\nmodel.load_state_dict(sd); model.to(device); model.eval()\ndef predict_mask(img_path, prompt, out_png, thr=0.5):\n    inp = processor(text=[prompt], images=[Image.open(img_path).convert(\"RGB\")], padding=\"max_length\", return_tensors=\"pt\")\n    out = model(pixel_values=inp[\"pixel_values\"].to(device),\n                input_ids=inp[\"input_ids\"].to(device),\n                attention_mask=inp[\"attention_mask\"].to(device))\n    prob = torch.sigmoid(out.logits)[0,0].cpu().numpy()\n    cv2.imwrite(out_png, (prob>thr).astype(\"uint8\")*255)\n# Example:\n# predict_mask(\"/kaggle/input/taping-cracks/taping/val/images/ANY.jpg\",\n#              \"segment taping area\",\n#              \"/kaggle/working/ANY__segment_taping_area.png\")\n\"\"\")\nprint(\"Helper written:\", SAVE_DIR/\"inference_helper.py\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T10:06:31.071760Z","iopub.execute_input":"2025-10-05T10:06:31.072464Z","iopub.status.idle":"2025-10-05T11:39:26.528538Z","shell.execute_reply.started":"2025-10-05T10:06:31.072432Z","shell.execute_reply":"2025-10-05T11:39:26.527774Z"}},"outputs":[{"name":"stdout","text":"Using DATA_ROOT: /kaggle/input/taping-cracks/data copy\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/15:   0%|          | 0/2932 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\nEpoch 1/15: 100%|██████████| 2932/2932 [06:45<00:00,  7.23it/s, avg_loss=0.3730]\nValid:   0%|          | 0/121 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 1] Val Dice=0.5670  mIoU=0.4179\nSaved: /kaggle/working/ckpts_clipseg/clipseg_best_e1_dice0.5670.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/15:   0%|          | 0/2932 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\nEpoch 2/15: 100%|██████████| 2932/2932 [05:32<00:00,  8.82it/s, avg_loss=0.3026]\nValid:   0%|          | 0/121 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 2] Val Dice=0.6219  mIoU=0.4707\nSaved: /kaggle/working/ckpts_clipseg/clipseg_best_e2_dice0.6219.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/15:   0%|          | 0/2932 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\nEpoch 3/15: 100%|██████████| 2932/2932 [05:32<00:00,  8.82it/s, avg_loss=0.2871]\nValid:   0%|          | 0/121 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 3] Val Dice=0.6413  mIoU=0.4921\nSaved: /kaggle/working/ckpts_clipseg/clipseg_best_e3_dice0.6413.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/15:   0%|          | 0/2932 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\nEpoch 4/15: 100%|██████████| 2932/2932 [05:31<00:00,  8.84it/s, avg_loss=0.2776]\nValid:   0%|          | 0/121 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 4] Val Dice=0.6503  mIoU=0.5018\nSaved: /kaggle/working/ckpts_clipseg/clipseg_best_e4_dice0.6503.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/15:   0%|          | 0/2932 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\nEpoch 5/15: 100%|██████████| 2932/2932 [05:32<00:00,  8.81it/s, avg_loss=0.2706]\nValid:   0%|          | 0/121 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 5] Val Dice=0.6510  mIoU=0.5023\nSaved: /kaggle/working/ckpts_clipseg/clipseg_best_e5_dice0.6510.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/15:   0%|          | 0/2932 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\nEpoch 6/15: 100%|██████████| 2932/2932 [05:32<00:00,  8.82it/s, avg_loss=0.2646]\nValid:   0%|          | 0/121 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 6] Val Dice=0.6584  mIoU=0.5109\nSaved: /kaggle/working/ckpts_clipseg/clipseg_best_e6_dice0.6584.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/15:   0%|          | 0/2932 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\nEpoch 7/15: 100%|██████████| 2932/2932 [05:32<00:00,  8.81it/s, avg_loss=0.2604]\nValid:   0%|          | 0/121 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 7] Val Dice=0.6577  mIoU=0.5096\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/15:   0%|          | 0/2932 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\nEpoch 8/15: 100%|██████████| 2932/2932 [05:32<00:00,  8.83it/s, avg_loss=0.2567]\nValid:   0%|          | 0/121 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 8] Val Dice=0.6619  mIoU=0.5151\nSaved: /kaggle/working/ckpts_clipseg/clipseg_best_e8_dice0.6619.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/15:   0%|          | 0/2932 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\nEpoch 9/15: 100%|██████████| 2932/2932 [05:31<00:00,  8.84it/s, avg_loss=0.2535]\nValid:   0%|          | 0/121 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 9] Val Dice=0.6629  mIoU=0.5153\nSaved: /kaggle/working/ckpts_clipseg/clipseg_best_e9_dice0.6629.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/15:   0%|          | 0/2932 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\nEpoch 10/15: 100%|██████████| 2932/2932 [05:32<00:00,  8.82it/s, avg_loss=0.2510]\nValid:   0%|          | 0/121 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 10] Val Dice=0.6650  mIoU=0.5177\nSaved: /kaggle/working/ckpts_clipseg/clipseg_best_e10_dice0.6650.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/15:   0%|          | 0/2932 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\nEpoch 11/15: 100%|██████████| 2932/2932 [05:32<00:00,  8.82it/s, avg_loss=0.2490]\nValid:   0%|          | 0/121 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 11] Val Dice=0.6668  mIoU=0.5199\nSaved: /kaggle/working/ckpts_clipseg/clipseg_best_e11_dice0.6668.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/15:   0%|          | 0/2932 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\nEpoch 12/15: 100%|██████████| 2932/2932 [05:32<00:00,  8.83it/s, avg_loss=0.2476]\nValid:   0%|          | 0/121 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\nEpoch 13/15: 100%|██████████| 2932/2932 [05:31<00:00,  8.84it/s, avg_loss=0.2463]\nValid:   0%|          | 0/121 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 13] Val Dice=0.6676  mIoU=0.5209\nSaved: /kaggle/working/ckpts_clipseg/clipseg_best_e13_dice0.6676.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/15:   0%|          | 0/2932 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\nEpoch 14/15: 100%|██████████| 2932/2932 [05:31<00:00,  8.85it/s, avg_loss=0.2464]\nValid:   0%|          | 0/121 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 14] Val Dice=0.6679  mIoU=0.5212\nSaved: /kaggle/working/ckpts_clipseg/clipseg_best_e14_dice0.6679.pt\n","output_type":"stream"},{"text":"Epoch 15/15:   0%|          | 0/2932 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\nEpoch 15/15:  42%|████▏     | 1221/2932 [02:19<03:14,  8.79it/s, avg_loss=0.2480]IOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=10000.0 (msgs/sec)\nNotebookApp.rate_limit_window=1.0 (secs)\n\n","name":"stderr","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# =========================\n# Test loop (same as val)\n# =========================\n# Assumes you already ran the training cell so the following exist:\n# - DATA_ROOT, SAVE_DIR, device, IMG_SIZE, BATCH, WORKERS\n# - processor, model\n# - CRACK_PROMPTS_VAL, TAPING_PROMPTS_VAL\n# - PromptedSegDataset, ConcatDataset, DataLoader\n# - dice_coeff, miou\n# - F from torch.nn.functional\n#\n# If you want to be sure you evaluate the best checkpoint, we (re)load it here.\nfrom pathlib import Path\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import ConcatDataset, DataLoader\nfrom tqdm import tqdm\n\n# ---- (Optional) reload best weights for testing ----\ntry:\n    # If best_path exists from training, prefer it; else fall back to final\n    ckpt_to_eval = best_path if ('best_path' in globals() and best_path is not None) else (SAVE_DIR / \"clipseg_final.pt\")\n    if Path(ckpt_to_eval).exists():\n        sd = torch.load(ckpt_to_eval, map_location=\"cpu\")\n        sd = sd.get(\"model\", sd)\n        model.load_state_dict(sd, strict=False)\n        model.to(device).eval()\n        print(\"Loaded checkpoint for TEST:\", ckpt_to_eval)\n    else:\n        print(\"No saved checkpoint found; using in-memory model weights.\")\nexcept Exception as e:\n    print(\"Checkpoint reload skipped due to error:\", e)\n\n# ---- Build TEST datasets exactly like VAL (no aug, fixed prompts) ----\nds_te_cr = PromptedSegDataset(str(DATA_ROOT/\"cracks\"/\"test\"),  CRACK_PROMPTS_VAL,  IMG_SIZE, True,  False, processor)\nds_te_tp = PromptedSegDataset(str(DATA_ROOT/\"taping\"/\"test\"), TAPING_PROMPTS_VAL, IMG_SIZE, False, False, processor)\nds_te = ConcatDataset([ds_te_cr, ds_te_tp])\ndl_te = DataLoader(ds_te, batch_size=BATCH, shuffle=False, num_workers=WORKERS, pin_memory=True)\n\n# ---- Evaluate (identical to your VAL loop) ----\nmodel.eval(); dices=[]; ious=[]\nwith torch.no_grad():\n    for b in tqdm(dl_te, leave=False, desc=\"Test\"):\n        px   = b[\"pixel_values\"].to(device, non_blocking=True)\n        ids  = b[\"input_ids\"].to(device, non_blocking=True)\n        attn = b[\"attention_mask\"].to(device, non_blocking=True)\n        gt   = b[\"mask\"].to(device, non_blocking=True)\n\n        out    = model(pixel_values=px, input_ids=ids, attention_mask=attn)\n        logits = out.logits\n        gt_rs  = F.interpolate(gt.unsqueeze(1), size=logits.shape[-2:], mode=\"nearest\").squeeze(1)\n        prob   = torch.sigmoid(logits)\n\n        dices.append(dice_coeff(prob, gt_rs))\n        ious.append(miou(prob, gt_rs))\n\nd_test = float(np.mean(dices)) if len(dices) else float(\"nan\")\nm_test = float(np.mean(ious))  if len(ious)  else float(\"nan\")\nprint(f\"[TEST] Dice={d_test:.4f}  mIoU={m_test:.4f}  (N={len(dices)})\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T12:19:22.729925Z","iopub.execute_input":"2025-10-05T12:19:22.730262Z","iopub.status.idle":"2025-10-05T12:19:32.259578Z","shell.execute_reply.started":"2025-10-05T12:19:22.730212Z","shell.execute_reply":"2025-10-05T12:19:32.258718Z"}},"outputs":[{"name":"stdout","text":"Loaded checkpoint for TEST: /kaggle/working/ckpts_clipseg/clipseg_best_e15_dice0.6681.pt\n","output_type":"stream"},{"name":"stderr","text":"Test:   0%|          | 0/28 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:44: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n  return self.preprocess(images, **kwargs)\n                                                     ","output_type":"stream"},{"name":"stdout","text":"[TEST] Dice=0.7106  mIoU=0.5625  (N=28)\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}