{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13267434,"sourceType":"datasetVersion","datasetId":8407603}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# SegFormer fine-tuning on /kaggle/input/taping-cracks (cracks + taping)\n# ➜ 3 classes: 0=bg, 1=crack, 2=taping\n# Logs per-epoch: loss, mIoU, Dice (classes 1&2), and saves best-by-mIoU.\n# No augmentations (only resize).\n\nimport os, glob, json, random\nfrom pathlib import Path\n\nimport numpy as np\nimport cv2\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\n\nfrom transformers import (\n    SegformerImageProcessor,\n    SegformerForSemanticSegmentation,\n    get_cosine_schedule_with_warmup,\n)\n\n# ======================\n# Config\n# ======================\nDATA_CANDIDATES = [\n    \"/kaggle/input/taping-cracks\",\n    \"/kaggle/input/taping-cracks/data copy\",\n    \"/kaggle/input/taping-cracks/data_copy\",\n]\nOUT_DIR   = Path(\"/kaggle/working/ckpts_segformer\")\nIMG_SIZE  = 512\nBATCH     = 6\nEPOCHS    = 20\nLR        = 5e-5\nWEIGHT_DECAY = 1e-4\nWARMUP_FRAC  = 0.05\nSEED = 42\nWORKERS = 2\nNUM_CLASSES = 3  # 0 bg, 1 crack, 2 taping\n\n# ======================\n# Utils\n# ======================\ndef set_seed(s=SEED):\n    random.seed(s); np.random.seed(s)\n    torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n    torch.backends.cudnn.benchmark=False; torch.backends.cudnn.deterministic=True\n\ndef find_data_root(candidates):\n    for base in candidates:\n        root = Path(base)\n        if (root/\"cracks\").exists() and (root/\"taping\").exists():\n            return root\n        if root.exists():\n            for sub in root.iterdir():\n                if sub.is_dir() and (sub/\"cracks\").exists() and (sub/\"taping\").exists():\n                    return sub\n    raise FileNotFoundError(\"Dataset not found in: \" + \", \".join(candidates))\n\ndef imread_rgb(path: str) -> np.ndarray:\n    arr = cv2.imread(path, cv2.IMREAD_COLOR)\n    if arr is None:\n        raise FileNotFoundError(path)\n    return cv2.cvtColor(arr, cv2.COLOR_BGR2RGB)\n\ndef read_mask_or_zero(mask_path: str, hw) -> np.ndarray:\n    \"\"\"Return uint8 mask (0/255). If missing, zeros of requested (H,W).\"\"\"\n    H, W = int(hw[0]), int(hw[1])\n    if mask_path and os.path.exists(mask_path):\n        m = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n        if m is None:\n            m = np.zeros((H, W), np.uint8)\n    else:\n        m = np.zeros((H, W), np.uint8)\n    # binarize\n    m = (m > 127).astype(np.uint8) * 255\n    return m\n\ndef resize_image_and_masks(img, crack_mask, taping_mask, size=IMG_SIZE):\n    img_r = cv2.resize(img, (size, size), interpolation=cv2.INTER_LINEAR)\n    ck_r  = cv2.resize(crack_mask, (size, size), interpolation=cv2.INTER_NEAREST)\n    tp_r  = cv2.resize(taping_mask, (size, size), interpolation=cv2.INTER_NEAREST)\n    return img_r, ck_r, tp_r\n\ndef build_semantic_label(ck_mask_255, tp_mask_255):\n    \"\"\"Return HxW int64 class map with {0,1,2}; taping overrides crack on overlaps.\"\"\"\n    ck = (ck_mask_255 > 127).astype(np.uint8)\n    tp = (tp_mask_255 > 127).astype(np.uint8)\n    lab = np.zeros_like(ck, dtype=np.uint8)\n    lab[ck == 1] = 1\n    lab[tp == 1] = 2\n    return lab.astype(np.int64)\n\n# -------- metrics (exclude background) --------\ndef batch_mean_iou_and_dice(pred_logits, labels, num_classes=NUM_CLASSES, eps=1e-6):\n    \"\"\"\n    pred_logits: [B,C,H,W]; labels: [B,H,W] int64.\n    Compute mIoU and Dice averaged over classes 1..num_classes-1 (exclude bg).\n    \"\"\"\n    with torch.no_grad():\n        pred = pred_logits.argmax(dim=1)  # [B,H,W]\n        B = pred.shape[0]\n        classes = list(range(1, num_classes))\n        ious, dices = [], []\n        for c in classes:\n            pred_c = (pred == c)\n            gt_c   = (labels == c)\n            inter  = (pred_c & gt_c).sum(dim=(1,2)).float()  # [B]\n            pred_sum = pred_c.sum(dim=(1,2)).float()\n            gt_sum   = gt_c.sum(dim=(1,2)).float()\n            union = pred_sum + gt_sum - inter\n            # For images where class absent in both, skip from average\n            valid = (union > 0)\n            if valid.any():\n                ious.append((inter[valid] / (union[valid] + eps)))\n            # Dice\n            denom = pred_sum + gt_sum\n            valid_d = (denom > 0)\n            if valid_d.any():\n                dices.append((2*inter[valid_d] / (denom[valid_d] + eps)))\n        # Mean over classes then over batch\n        miou = torch.stack([x.mean() for x in ious]).mean().item() if ious else 0.0\n        dice = torch.stack([x.mean() for x in dices]).mean().item() if dices else 0.0\n        return miou, dice\n\n# ======================\n# Dataset\n# ======================\nclass CrackTapingSemSeg(Dataset):\n    \"\"\"\n    Expects:\n      <root>/\n        cracks/train/images/*.jpg (or png)\n        cracks/train/masks/*.png\n        taping/train/images/*.jpg\n        taping/train/masks/*.png\n      and same for 'val'.\n    We build a unified image list from both tasks; mask paths are resolved per image name.\n    \"\"\"\n    def __init__(self, root, split=\"train\", img_size=IMG_SIZE):\n        self.root  = Path(root)\n        self.split = split\n        self.size  = img_size\n\n        self.cr_img_dir = self.root/\"cracks\"/split/\"images\"\n        self.cr_msk_dir = self.root/\"cracks\"/split/\"masks\"\n        self.tp_img_dir = self.root/\"taping\"/split/\"images\"\n        self.tp_msk_dir = self.root/\"taping\"/split/\"masks\"\n\n        # Gather all image paths from both sub-datasets\n        self.items = []\n        for d in [self.cr_img_dir, self.tp_img_dir]:\n            for ip in sorted(glob.glob(str(d/\"*\"))):\n                self.items.append(ip)\n        if not self.items:\n            raise RuntimeError(f\"No images found under {self.cr_img_dir} and {self.tp_img_dir}\")\n\n        # For mask lookup, we’ll try the same stem in both mask dirs (png/jpg/jpeg)\n        self.mask_exts = (\".png\", \".jpg\", \".jpeg\")\n\n        # Minimal processor just to normalize/format pixel_values\n        self.proc = SegformerImageProcessor.from_pretrained(\n            \"nvidia/segformer-b2-finetuned-ade-512-512\"\n        )\n\n    def _find_mask(self, mdir: Path, stem: str):\n        for ext in self.mask_exts:\n            cand = mdir / f\"{stem}{ext}\"\n            if cand.exists():\n                return str(cand)\n        return None\n\n    def __len__(self): return len(self.items)\n\n    def __getitem__(self, idx):\n        ip = self.items[idx]  # <-- keep name consistent to avoid NameError\n        img = imread_rgb(ip)\n        H, W = img.shape[:2]\n\n        stem = Path(ip).stem\n        # Resolve masks from both categories (could be None)\n        ck_mask_path = self._find_mask(self.cr_msk_dir, stem)\n        tp_mask_path = self._find_mask(self.tp_msk_dir, stem)\n\n        # Read or zero, then resize\n        crack = read_mask_or_zero(ck_mask_path, (H, W))\n        taping= read_mask_or_zero(tp_mask_path, (H, W))\n        img_r, crack_r, taping_r = resize_image_and_masks(img, crack, taping, self.size)\n\n        # Merge into class map (0/1/2)\n        label = build_semantic_label(crack_r, taping_r)  # int64 [H,W]\n\n        # Processor to float + normalize to pixel_values [3,H,W]\n        # (We pass a PIL Image to keep consistency with HF preproc expectations)\n        enc = self.proc(images=Image.fromarray(img_r), return_tensors=\"pt\")\n        pixel_values = enc[\"pixel_values\"][0]  # [3,H,W], float\n\n        return {\n            \"pixel_values\": pixel_values,\n            \"labels\": torch.from_numpy(label).long(),  # [H,W]\n            \"path\": ip,\n        }\n\n# ======================\n# Train\n# ======================\nset_seed(SEED)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\nDATA_ROOT = find_data_root(DATA_CANDIDATES)\nprint(\"Using DATA_ROOT:\", DATA_ROOT)\n\n# Datasets / loaders\nds_tr = CrackTapingSemSeg(DATA_ROOT, \"train\", IMG_SIZE)\nds_va = CrackTapingSemSeg(DATA_ROOT, \"val\",   IMG_SIZE)\n\ndl_tr = DataLoader(ds_tr, batch_size=BATCH, shuffle=True,  num_workers=WORKERS,\n                   pin_memory=True, drop_last=True)\ndl_va = DataLoader(ds_va, batch_size=BATCH, shuffle=False, num_workers=WORKERS,\n                   pin_memory=True)\n\n# Model\nid2label = {0: \"background\", 1: \"crack\", 2: \"taping\"}\nlabel2id = {v:k for k,v in id2label.items()}\nmodel = SegformerForSemanticSegmentation.from_pretrained(\n    \"nvidia/segformer-b2-finetuned-ade-512-512\",\n    num_labels=NUM_CLASSES,\n    id2label=id2label, label2id=label2id,\n    ignore_mismatched_sizes=True,  # ADE head is 150 classes\n).to(device)\n\n# Optim + sched\nopt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\nsteps_total = EPOCHS * len(dl_tr)\nwarm = max(1, int(WARMUP_FRAC * steps_total))\nsched = get_cosine_schedule_with_warmup(opt, num_warmup_steps=warm, num_training_steps=steps_total)\n\nuse_amp = torch.cuda.is_available()\nscaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n\nbest_miou, best_ckpt = -1.0, None\n\nfor epoch in range(1, EPOCHS+1):\n    model.train()\n    running = 0.0\n    pbar = tqdm(dl_tr, desc=f\"Epoch {epoch}/{EPOCHS}\", total=len(dl_tr))\n    for step, batch in enumerate(pbar, 1):\n        pixel_values = batch[\"pixel_values\"].to(device, non_blocking=True)  # [B,3,H,W]\n        labels       = batch[\"labels\"].to(device, non_blocking=True)        # [B,H,W]\n        opt.zero_grad(set_to_none=True)\n        with torch.cuda.amp.autocast(enabled=use_amp):\n            out = model(pixel_values=pixel_values, labels=labels)\n            loss = out.loss\n        scaler.scale(loss).backward()\n        scaler.step(opt); scaler.update()\n        sched.step()\n        running += loss.item()\n        pbar.set_postfix(loss=f\"{running/step:.4f}\")\n\n    # ---- validation (mIoU & Dice over classes 1,2) ----\n    model.eval()\n    miou_list, dice_list = [], []\n    with torch.no_grad():\n        for batch in tqdm(dl_va, leave=False, desc=\"Valid\"):\n            pixel_values = batch[\"pixel_values\"].to(device)\n            labels       = batch[\"labels\"].to(device)\n            with torch.cuda.amp.autocast(enabled=use_amp):\n                logits = model(pixel_values=pixel_values).logits  # [B,C,h,w]\n            # Upsample logits to labels size if needed\n            if logits.shape[-2:] != labels.shape[-2:]:\n                logits = F.interpolate(logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n            mi, di = batch_mean_iou_and_dice(logits, labels, NUM_CLASSES)\n            miou_list.append(mi); dice_list.append(di)\n    m_miou = float(np.mean(miou_list)) if miou_list else 0.0\n    m_dice = float(np.mean(dice_list)) if dice_list else 0.0\n    print(f\"[Epoch {epoch}] Val mIoU={m_miou:.4f}  Dice={m_dice:.4f}\")\n\n    # Save best-by-mIoU\n    if m_miou > best_miou:\n        best_miou = m_miou\n        best_ckpt = OUT_DIR / f\"segformer_b2_best_e{epoch}_miou{m_miou:.4f}.pt\"\n        torch.save({\n            \"model\": model.state_dict(),\n            \"epoch\": epoch,\n            \"val_miou\": m_miou,\n            \"val_dice\": m_dice,\n            \"id2label\": id2label,\n        }, best_ckpt)\n        print(\"Saved:\", best_ckpt)\n\n# Save final\nfinal_ckpt = OUT_DIR / \"segformer_b2_final.pt\"\ntorch.save({\"model\": model.state_dict(), \"id2label\": id2label}, final_ckpt)\nprint(\"Final:\", final_ckpt, \"| Best:\", best_ckpt)\n\n# Tiny inference helper\nhelper = OUT_DIR / \"inference_helper.py\"\nhelper.write_text(f\"\"\"import torch, cv2\nimport numpy as np\nfrom PIL import Image\nfrom transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nproc = SegformerImageProcessor.from_pretrained(\"nvidia/segformer-b2-finetuned-ade-512-512\")\nckpt = r\"{str(best_ckpt if best_ckpt else final_ckpt)}\"\nstate = torch.load(ckpt, map_location=\"cpu\")\nid2label = state.get(\"id2label\", {{0: \"background\", 1: \"crack\", 2: \"taping\"}})\nmodel = SegformerForSemanticSegmentation.from_pretrained(\n    \"nvidia/segformer-b2-finetuned-ade-512-512\", num_labels=len(id2label),\n    ignore_mismatched_sizes=True, id2label=id2label, label2id={{v:k for k,v in id2label.items()}}\n)\nmodel.load_state_dict(state[\"model\"], strict=False)\nmodel.to(device).eval()\n\ndef predict_semantic(img_path, out_png, img_size={IMG_SIZE}):\n    img = Image.open(img_path).convert(\"RGB\").resize((img_size, img_size), resample=Image.BILINEAR)\n    enc = proc(images=img, return_tensors=\"pt\")\n    with torch.no_grad():\n        logits = model(pixel_values=enc[\"pixel_values\"].to(device)).logits\n    # upsample to processor size (already img_size)\n    logits = torch.nn.functional.interpolate(logits, size=(img_size, img_size), mode=\"bilinear\", align_corners=False)\n    pred = logits.argmax(dim=1)[0].cpu().numpy().astype(np.uint8)\n    # write colorful visualization for quick checking\n    palette = np.array([[0,0,0],[255,0,0],[0,255,0]], dtype=np.uint8)\n    vis = palette[pred]\n    cv2.imwrite(out_png, cv2.cvtColor(vis, cv2.COLOR_RGB2BGR))\n\n# Example:\n# predict_semantic(\"/kaggle/input/taping-cracks/cracks/val/images/ANY.jpg\", \"/kaggle/working/ANY_segformer_vis.png\")\n\"\"\")\nprint(\"Helper written:\", helper)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-05T20:30:25.066059Z","iopub.execute_input":"2025-10-05T20:30:25.066300Z","iopub.status.idle":"2025-10-06T02:44:46.421298Z","shell.execute_reply.started":"2025-10-05T20:30:25.066279Z","shell.execute_reply":"2025-10-06T02:44:46.420084Z"}},"outputs":[{"name":"stdout","text":"Using DATA_ROOT: /kaggle/input/taping-cracks/data copy\n","output_type":"stream"},{"name":"stderr","text":"Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b2-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n- decode_head.classifier.weight: found shape torch.Size([150, 768, 1, 1]) in the checkpoint and torch.Size([3, 768, 1, 1]) in the model instantiated\n- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([3]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_36/3902088299.py:242: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\nEpoch 1/20:   0%|          | 0/2932 [00:00<?, ?it/s]/tmp/ipykernel_36/3902088299.py:254: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=use_amp):\nEpoch 1/20: 100%|██████████| 2932/2932 [18:33<00:00,  2.63it/s, loss=0.3210]\nValid:   0%|          | 0/121 [00:00<?, ?it/s]/tmp/ipykernel_36/3902088299.py:270: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=use_amp):\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 1] Val mIoU=0.4356  Dice=0.5712\nSaved: /kaggle/working/ckpts_segformer/segformer_b2_best_e1_miou0.4356.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/20: 100%|██████████| 2932/2932 [18:26<00:00,  2.65it/s, loss=0.0820]\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 2] Val mIoU=0.5303  Dice=0.6667\nSaved: /kaggle/working/ckpts_segformer/segformer_b2_best_e2_miou0.5303.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/20: 100%|██████████| 2932/2932 [18:20<00:00,  2.66it/s, loss=0.0638]\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 3] Val mIoU=0.5833  Dice=0.7084\nSaved: /kaggle/working/ckpts_segformer/segformer_b2_best_e3_miou0.5833.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/20: 100%|██████████| 2932/2932 [18:22<00:00,  2.66it/s, loss=0.0542]\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 4] Val mIoU=0.5853  Dice=0.7120\nSaved: /kaggle/working/ckpts_segformer/segformer_b2_best_e4_miou0.5853.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/20: 100%|██████████| 2932/2932 [18:25<00:00,  2.65it/s, loss=0.0478]\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 5] Val mIoU=0.5931  Dice=0.7149\nSaved: /kaggle/working/ckpts_segformer/segformer_b2_best_e5_miou0.5931.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/20: 100%|██████████| 2932/2932 [18:22<00:00,  2.66it/s, loss=0.0427]\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 6] Val mIoU=0.6097  Dice=0.7334\nSaved: /kaggle/working/ckpts_segformer/segformer_b2_best_e6_miou0.6097.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/20: 100%|██████████| 2932/2932 [18:24<00:00,  2.66it/s, loss=0.0387]\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 7] Val mIoU=0.6281  Dice=0.7482\nSaved: /kaggle/working/ckpts_segformer/segformer_b2_best_e7_miou0.6281.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/20: 100%|██████████| 2932/2932 [18:22<00:00,  2.66it/s, loss=0.0356]\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 8] Val mIoU=0.6317  Dice=0.7528\nSaved: /kaggle/working/ckpts_segformer/segformer_b2_best_e8_miou0.6317.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/20: 100%|██████████| 2932/2932 [18:24<00:00,  2.65it/s, loss=0.0329]\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 9] Val mIoU=0.6410  Dice=0.7570\nSaved: /kaggle/working/ckpts_segformer/segformer_b2_best_e9_miou0.6410.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/20: 100%|██████████| 2932/2932 [18:22<00:00,  2.66it/s, loss=0.0283]\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 11] Val mIoU=0.6318  Dice=0.7494\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/20: 100%|██████████| 2932/2932 [18:21<00:00,  2.66it/s, loss=0.0267]\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 12] Val mIoU=0.6513  Dice=0.7642\nSaved: /kaggle/working/ckpts_segformer/segformer_b2_best_e12_miou0.6513.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/20: 100%|██████████| 2932/2932 [18:25<00:00,  2.65it/s, loss=0.0252]\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 13] Val mIoU=0.6540  Dice=0.7668\nSaved: /kaggle/working/ckpts_segformer/segformer_b2_best_e13_miou0.6540.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/20: 100%|██████████| 2932/2932 [18:28<00:00,  2.65it/s, loss=0.0240]\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 14] Val mIoU=0.6536  Dice=0.7664\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/20: 100%|██████████| 2932/2932 [18:25<00:00,  2.65it/s, loss=0.0230]\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 15] Val mIoU=0.6576  Dice=0.7700\nSaved: /kaggle/working/ckpts_segformer/segformer_b2_best_e15_miou0.6576.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/20: 100%|██████████| 2932/2932 [18:25<00:00,  2.65it/s, loss=0.0223]\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 16] Val mIoU=0.6560  Dice=0.7684\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/20: 100%|██████████| 2932/2932 [18:28<00:00,  2.64it/s, loss=0.0217]\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 17] Val mIoU=0.6570  Dice=0.7686\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/20: 100%|██████████| 2932/2932 [18:29<00:00,  2.64it/s, loss=0.0213]\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 18] Val mIoU=0.6570  Dice=0.7688\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/20: 100%|██████████| 2932/2932 [18:27<00:00,  2.65it/s, loss=0.0210]\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 19] Val mIoU=0.6591  Dice=0.7706\nSaved: /kaggle/working/ckpts_segformer/segformer_b2_best_e19_miou0.6591.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/20: 100%|██████████| 2932/2932 [18:26<00:00,  2.65it/s, loss=0.0210]\n                                                        \r","output_type":"stream"},{"name":"stdout","text":"[Epoch 20] Val mIoU=0.6581  Dice=0.7698\nFinal: /kaggle/working/ckpts_segformer/segformer_b2_final.pt | Best: /kaggle/working/ckpts_segformer/segformer_b2_best_e19_miou0.6591.pt\nHelper written: /kaggle/working/ckpts_segformer/inference_helper.py\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}